[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Instalación de Paquetes",
    "section": "",
    "text": "En esta sección encontraras los requisitos previos del Curso de Genómica antes de empezar. Este tutorial contempla principalmente el SO: Windows y Linux: Ubuntu 24"
  },
  {
    "objectID": "index.html#instalación-de-r-y-rstudio-para-windows",
    "href": "index.html#instalación-de-r-y-rstudio-para-windows",
    "title": "Instalación de Paquetes",
    "section": "Instalación de R y Rstudio para Windows",
    "text": "Instalación de R y Rstudio para Windows\n\n\n\n\n\n\n\nNota\n\n\n\nSi cuenta con WSL (Windows subsystem Linux) instale tanto R como Rstudio en Windows!."
  },
  {
    "objectID": "index.html#instalación-de-r-y-rstudio-para-linux",
    "href": "index.html#instalación-de-r-y-rstudio-para-linux",
    "title": "Instalación de Paquetes",
    "section": "Instalación de R y Rstudio para Linux",
    "text": "Instalación de R y Rstudio para Linux\n\n\n\n\n\n\n\nNota\n\n\n\nSi quiere instalar sin incovenientes verifique las versión de Linux que cuenta. Si desea ver mas opciones de instalación puede ir a la página de descargas de Rstudio Desktop"
  },
  {
    "objectID": "index.html#instalaciones-de-paquetes-de-r",
    "href": "index.html#instalaciones-de-paquetes-de-r",
    "title": "Instalación de Paquetes",
    "section": "Instalaciones de paquetes de R",
    "text": "Instalaciones de paquetes de R\nPara instalar los paquetes de R usaremos el gestor de paquetes de Bioconductor.\n## Instalar gestor de paquetes de Bioconductor\ninstall.packages(\"BiocManager\")\n\n## Activar BiocManager\nlibrary(BiocManager)\nLos paquetes pueden variar dependiendo del desarrollo de las clases\n## Instalar los siguientes paquetes:\n\nBiocManager::install(\"dplyr\")\nBiocManager::install(\"ggplot2\")\nBiocManager::install(\"Rbowtie2\")\nBiocManager::install(\"Rsamtools\")\nBiocManager::install(\"ape\")\nBiocManager::install(\"rjson\")\nBiocManager::install(\"foreach\")\nBiocManager::install(\"doParallel\")\nBiocManager::install(\"tidyr\")\nBiocManager::install(\"fastqcr\")\nBiocManager::install(\"Biostrings\")\nEn caso tenga el SO Linux y tenga problemas con la instalación comunicarse con los asesores del curso para que puedan ayudarle en la instalación."
  },
  {
    "objectID": "index.html#instalaciones-de-programas-de-linux",
    "href": "index.html#instalaciones-de-programas-de-linux",
    "title": "Instalación de Paquetes",
    "section": "Instalaciones de programas de Linux",
    "text": "Instalaciones de programas de Linux\n\n\n\n\n\n\nInstalar Linux dentro de Windows\n\n\n\nSi cuenta con Windows 10 / 11 puede instalar WSL (Windows Subsystem Linux) puede ir al siguiente video tutorial Instalacion WSL\n\n\nPara el análasis de datos en Linux se instalaran los siguiente programas:\nsudo apt-get install jellyfish bwa samtools ivar prokka\njellyfish : Conteo de k-mers\nbwa : Alineador de datos NGS fastq\nsamtools : Manejo de secuencias alineadas SAM/BAM\nivar : Alineador de datos NGS fastq\nprokka : Annotador de genomas bacterianos\n\nInstalación a travez de conda\nSi tiene problemas instalando los programas de linux recomendamos instalar los paquetes necesarios a travez de conda (Anaconda), para ello pueden ir a la pagina de Anaconda o puedes ejecutar el script de instalación de la siguiente manera :\nwget https://repo.anaconda.com/archive/Anaconda3-2023.09-0-Linux-x86_64.sh\nsudo bash Anaconda3-2023.03-Linux-x86_64.sh\nUna vez que haya instalado Anaconda con exito instalar los paquetes del curso de la siguiente manera\n### Activar conda si no se activo inicialmente\n### conda activate\n\nwget  https://raw.githubusercontent.com/FranciscoAscue/Rgenomics/master/ngs_conda.yml\nconda env create -f ngs_conda.yml"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "modulo6.html",
    "href": "modulo6.html",
    "title": "Introducción a R",
    "section": "",
    "text": "R es un lenguaje de programación.\nR es un software de código abierto y libre.\nCuando instalamos R, se instala su núcleo básico.\n\n\n\n\n\n\n\nNota\n\n\n\n\n\nSin embargo, gran parte de sus funciones adicionales se encuentran en lo que se conocen como “paquetes”",
    "crumbs": [
      "Introduccion a R",
      "Introducción a R"
    ]
  },
  {
    "objectID": "modulo6.html#qué-es-r",
    "href": "modulo6.html#qué-es-r",
    "title": "Introducción a R",
    "section": "",
    "text": "R es un lenguaje de programación.\nR es un software de código abierto y libre.\nCuando instalamos R, se instala su núcleo básico.\n\n\n\n\n\n\n\nNota\n\n\n\n\n\nSin embargo, gran parte de sus funciones adicionales se encuentran en lo que se conocen como “paquetes”",
    "crumbs": [
      "Introduccion a R",
      "Introducción a R"
    ]
  },
  {
    "objectID": "modulo6.html#por-qué-r",
    "href": "modulo6.html#por-qué-r",
    "title": "Introducción a R",
    "section": "¿Por qué R?",
    "text": "¿Por qué R?\n\nPlataforma Versátil: R es compatible con una amplia gama de sistemas operativos, incluyendo Mac, Windows y Linux.\nLenguaje de Programación Completo: R es más que un simple software estadístico.\nPromoción de la Investigación Reproducible: R no solo facilita la realización de análisis de datos, sino que también fomenta la investigación reproducible.\nComunidad Activa: R se mantiene actualizado gracias a su comunidad activa de usuarios y desarrolladores. Con cerca de 19950 paquetes disponibles en CRAN (Comprehensive R Archive Network).\nIntegración con Herramientas Bioinformáticas: R se integra de manera efectiva con herramientas bioinformáticas y formatos de datos procesados.\nCapacidades Gráficas Sofisticadas: R ofrece capacidades gráficas avanzadas que te permiten crear visualizaciones atractivas y sofisticadas para representar tus datos de manera efectiva.\nPopular en Estadísticas y Bioinformática: R es ampliamente reconocido y utilizado en la comunidad estadística, y su popularidad sigue creciendo en el ámbito de la bioinformática.",
    "crumbs": [
      "Introduccion a R",
      "Introducción a R"
    ]
  },
  {
    "objectID": "modulo6.html#cómo-entender-r",
    "href": "modulo6.html#cómo-entender-r",
    "title": "Introducción a R",
    "section": "¿Cómo entender R?",
    "text": "¿Cómo entender R?\n\nHay una sesión de R corriendo. La consola de R es la interfaz entre R y nosotros.\nEn la sesión hay objetos. Todo en R es un objeto: vectores, tablas, funciones, etc.\nOperamos aplicando funciones a los objetos y creando nuevos objetos.",
    "crumbs": [
      "Introduccion a R",
      "Introducción a R"
    ]
  },
  {
    "objectID": "modulo6.html#la-consola-o-terminal",
    "href": "modulo6.html#la-consola-o-terminal",
    "title": "Introducción a R",
    "section": "La consola (o terminal)",
    "text": "La consola (o terminal)\nEs una ventana que nos permite comunicarnos al motor de R. Esta ventana acepta comandos en el lenguaje de R y brinda una respuesta (resultado) a dichos comandos.\nPor ejemplo, le podemos pedir a R que sume 1+1 así:\n1+1\nLa consola se distingue por tener el símbolo &gt; seguido de un cursor parpadeante que espera a que le demos instrucciones (cuando recién abrimos R además muestra la versión que tenemos instalada y otra info).\nTu Consola debe verse más o menos así después del ejemplo anterior:\n\n\n\nConsola",
    "crumbs": [
      "Introduccion a R",
      "Introducción a R"
    ]
  },
  {
    "objectID": "modulo6.html#scripts-y-el-editor",
    "href": "modulo6.html#scripts-y-el-editor",
    "title": "Introducción a R",
    "section": "Scripts y el editor",
    "text": "Scripts y el editor\nUn script es un archivo de nuestros análisis que es:\n\nUn archivo de texto plano\nPermanente,\nRepetible,\nAnotado y\nCompartible\n\n\n\n\n\n\n\nNota\n\n\n\n\n\nEn otras palabras, un script es una recopilación por escrito de las instrucciones que queremos enviar a la consola, de modo que al tener esas instrucciones cualquiera pueda repetir el análisis tal cual se hizo.\n\n\n\nUn script muy sencillo podría verse así:\n\n\n\nScript\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nLa idea es que en el editor de texto vayas escribiendo los comandos y comentarios de tu script hasta que haga exactamente lo que quieras.",
    "crumbs": [
      "Introduccion a R",
      "Introducción a R"
    ]
  },
  {
    "objectID": "modulo6.html#r-basico",
    "href": "modulo6.html#r-basico",
    "title": "Introducción a R",
    "section": "R Basico",
    "text": "R Basico\nAntes de adentrarnos en las funciones específicas para bioinformática, es fundamental comprender la sintaxis básica de R y los comandos esenciales que necesitas conocer. A continuación, se resumen estos conceptos clave:\n\nExpresiones Matemáticas: Puedes realizar cálculos matemáticos simples, por ejemplo, 1+1.\nStrings de Texto: R permite trabajar con texto, como en el ejemplo \"¡Holaaaaa mundo!\".\nValores Lógicos: Puedes evaluar expresiones lógicas, por ejemplo, 1&lt;5 o 2+2 == 5.\nCreación de Variables: Para almacenar datos, puedes crear variables u objetos, como en x &lt;- 5.\nFunciones: Las funciones son comandos que realizan tareas específicas en R.\n\nEjemplos de funciones comunes en R incluyen sum(), mean(), y log(). Para obtener ayuda detallada sobre la función log(), puedes utilizar el comando ?log.\n\nMatrices\n\nMatrices matrix(0, 3, 5)\nAcceso a elementos e una matriz [ , ]\n\n\n\nData frames\n\nData frame data.frame(x = c(\"a\", \"b\", \"c\"), y = 1:3)\nAcceso a elementos e una data.frame [ , ], $",
    "crumbs": [
      "Introduccion a R",
      "Introducción a R"
    ]
  },
  {
    "objectID": "modulo6.html#cargar-archivos-y-configurar-wd",
    "href": "modulo6.html#cargar-archivos-y-configurar-wd",
    "title": "Introducción a R",
    "section": "Cargar Archivos y Configurar WD",
    "text": "Cargar Archivos y Configurar WD\n\nCargar Archivos\nPara cargar archivos de texto con filas y columnas en R, puedes utilizar la función read.table(). Cuando utilices read.table() u otras funciones similares, asume que tu Directorio de Trabajo (WD) es la ubicación donde reside tu script y utiliza rutas relativas para acceder a los archivos que deseas cargar.\n\n\nConfigurar el Directorio de Trabajo (WD)\nEs una buena práctica que tu Directorio de Trabajo sea la ubicación donde reside el script en el que estás trabajando. Para determinar cuál es tu Directorio de Trabajo actual en R, puedes utilizar la función getwd().\nSi necesitas cambiar manualmente tu Directorio de Trabajo, utiliza la función setwd(). Sin embargo, ten en cuenta que debes realizar esta acción en la Consola de R, no en tu script.\n\n\n\n\n\n\nTip\n\n\n\n\n\nUna trampa útil en RStudio para establecer automáticamente tu Directorio de Trabajo en la ubicación de tu script es seleccionar la opción: Session &gt; Set Working Directory &gt; To source file location",
    "crumbs": [
      "Introduccion a R",
      "Introducción a R"
    ]
  },
  {
    "objectID": "modulo6.html#uso-de-source",
    "href": "modulo6.html#uso-de-source",
    "title": "Introducción a R",
    "section": "Uso de source():",
    "text": "Uso de source():\n\nLa función source() te permite ejecutar un script de R dentro de otro script de R. Esto es útil para modularizar tu código y reutilizar funciones personalizadas. Puedes llamar a un script externo utilizando source() de la siguiente manera:\n\nsource(\"nombre_del_script.r\")\n\n\n\n\n\n\nPrecaución\n\n\n\n\n\nAsegúrate de que tu Directorio de Trabajo getwd() sea la ubicación correcta para el archivo que deseas cargar.",
    "crumbs": [
      "Introduccion a R",
      "Introducción a R"
    ]
  },
  {
    "objectID": "modulo6.html#crear-una-función-propia",
    "href": "modulo6.html#crear-una-función-propia",
    "title": "Introducción a R",
    "section": "Crear una Función Propia:",
    "text": "Crear una Función Propia:\n\nPara crear tu propia función en R, sigue el siguiente esqueleto básico:\n\nnombre_de_tu_funcion &lt;- function(arg1, arg2, ...) {\n  # Declaraciones y operaciones\n  return(resultado)\n}\n\nnombre_de_tu_funcion: Es el nombre que asignas a tu función.\narg1, arg2, ...: Representan los argumentos que tu función acepta.\n# Declaraciones y operaciones: Aquí defines las operaciones que deseas realizar dentro de la función.\nreturn(resultado): Utiliza return() al final de la función si deseas que la función devuelva un resultado. De lo contrario, la función se ejecutará pero no devolverá ningún valor.",
    "crumbs": [
      "Introduccion a R",
      "Introducción a R"
    ]
  },
  {
    "objectID": "modulo6.html#ejemplo-de-función-propia",
    "href": "modulo6.html#ejemplo-de-función-propia",
    "title": "Introducción a R",
    "section": "Ejemplo de Función Propia:",
    "text": "Ejemplo de Función Propia:\n\nAquí tienes un ejemplo de una función propia que hace un conteo de los datos dentro de un array:\n\nconteo_datos &lt;- function(array) {\n   x &lt;- table(array)\n   return(x)\n}\n\n\n\n\n\n\nNota\n\n\n\n\n\nAl modularizar tu código utilizando funciones propias y source(), puedes mejorar la organización de tu análisis, reutilizar código y compartir funciones útiles con otros usuarios.",
    "crumbs": [
      "Introduccion a R",
      "Introducción a R"
    ]
  },
  {
    "objectID": "modulo7.html",
    "href": "modulo7.html",
    "title": "Fundamentos de NGS",
    "section": "",
    "text": "¿Qué son los datos de secuenciación de nueva generación (NGS)?\nLos datos NGS son las secuencias tal como provienen directamente de la plataforma de secuenciación en su forma cruda que a menudo contienen cierto grado de información no deseada:",
    "crumbs": [
      "Fundamentos NGS",
      "Fundamentos de NGS"
    ]
  },
  {
    "objectID": "modulo7.html#archivos-fastq",
    "href": "modulo7.html#archivos-fastq",
    "title": "Fundamentos de NGS",
    "section": "Archivos FASTQ",
    "text": "Archivos FASTQ\n\nRepresentación de secuencias\nEn informática las secuencias de ADN son una string (cadena) de caracteres.\n\nSecuencias genómicas {A,C,G,T}+\nSecuencias mRNA {A,C,G,U}+\n\n\n\nSecuencias simples: FASTA\nLínea 1: información de la secuencia\nLínea 2: la secuencia.\n\n\n\n\n\n\nEjemplo de Fasta\n\n\n\n\n\n&gt;gi|365266830|gb|JF701598.1| Pinus densiflora var. densiflora voucher Pf0855 trnS-trnG intergenic spacer, partial sequence; chloroplast\nGCAGAAAAAATCAGCAGTCATACAGTGCTTGACCTAATTTGATAGCTAGAATACCTGCTGTAAAAGCAAG\nAAAAAAAGCTATCAAAAATTTAAGCTCTACCATATCTTCATTCCCTCCTCAATGAGTTTGATTAAATGCG\nTTACATGGATTAGTCCATTTATTTCTCTCCAATATCAAATTTTATTATCTAGATATTGAAGGGTTCTCTA\nTCTATTTTATTATTATTGTAACGCTATCAGTTGCTCAAGGCCATAGGTTCCTGATCGAAACTACACCAAT\nGGGTAGGAGTCCGAAGAAGACAAAATAGAAGAAAAGTGATTGATCCCGACAACATTTTATTCATACATTC\nAGTCGATGGAGGGTGAAAGAAAACCAAATGGATCTAGAAGTTATTGCGCAGCTCACTGTTCTGACTCTGA\nTGGTTGTATCGGGCCCTTTAGTTATTGTTTTATCAGCAATTCGCAAAGGTAATCTATAATTACAATGAGC\nCATCTCCGGAGATGGCTCATTGTAATGATGAAAACGAGGTAATGATTGATATAAACTTTCAATAGAGGTT\nGATTGATAACTCCTCATCTTCCTATTGGTTGGACAAAAGATCGATCCA\n\n\n\n\n\nFastq: Illumina Reads\nSecuencia fasta + detalles calidad de la información (la Q es de Quality).\n\nLínea 1: Encabezado (Header): comienza con @.\nLínea 2: la secuencia.\nLínea 3: Comienza con +. Puede ser sólo el símbolo + o repetir la info del Header.\nLínea 4: Información de la calidad de secuenciación de cada base. Cada letra o símbolo representa a una base de la secuencia codificado en formato ASCII.\n\nLa info de calidad se codifica en ASCII porque esto permite en 1 sólo caracter codificar un valor de calidad. Por lo tanto la línea 2 y la 4 tienen el mismo número de caracteres.\nOrdenados de menor a mayor estos son los caracteres ASCII usados para representar calidad en FASTQ:\n!\"#$%&'()*+,-./0123456789:;&lt;=&gt;?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~\n\n\n\n\n\n\nOjo\n\n\n\n@ y + están dentro de los caracteres ASCII utilizados para codificar la calidad.\n\n\n\n\n\n\n\n\nEjemplos de Fastq\n\n\n\n\n\nEjemplos:\nEjemplo de datos FASTQ recién salidos de Illumina:\n@HWI-ST999:102:D1N6AACXX:1:1101:1235:1936 1:N:0:\nATGTCTCCTGGACCCCTCTGTGCCCAAGCTCCTCATGCATCCTCCTCAGCAACTTGTCCTGTAGCTGAGGCTCACTGACTACCAGCTGCAG\n+\n1:DAADDDF&lt;B&lt;AGF=FGIEHCCD9DG=1E9?D&gt;CF@HHG??B&lt;GEBGHCG;;CDB8==C@@&gt;&gt;GII@@5?A?@B&gt;CEDCFCC:;?CCCAC\n@OBIWAN:24:D1KUMACXX:3:1112:9698:62774 1:N:0:\nTAATATGGCTAATGCCCTAATCTTAGTGTGCCCAACCCACTTACTAACAAATAACTAACATTAAGATCGGAAGAGCACACGTCTGAACTCAGTCACTGACC\n+\nCCCFFFFFHHHHHIJJJJJJJJJJJJIIHHIJJJJJJJJJJJJJJJJJJJJIJJJJJJIJJJJIJJJJJJJHHHHFDFFEDEDDDDDDDDDDDDDDDDDDC\n¿Quieres saber cuáles son las partes del Header? Clic aquí.\nEjemplo de datos FASTQ del SRA:\n@SRR001666.1 071112_SLXA-EAS1_s_7:5:1:817:345 length=36\nGGGTGATGGCCGCTGCCGATGGCGTCAAATCCCACC\n+SRR001666.1 071112_SLXA-EAS1_s_7:5:1:817:345 length=36\nIIIIIIIIIIIIIIIIIIIIIIIIIIIIII9IG9IC\nLos datos FASTQ típicamente están comprimidos en formato gzip (.gz) o tar (.tar.gz o .tgz).",
    "crumbs": [
      "Fundamentos NGS",
      "Fundamentos de NGS"
    ]
  },
  {
    "objectID": "modulo7.html#análisis-de-datos-ngs",
    "href": "modulo7.html#análisis-de-datos-ngs",
    "title": "Fundamentos de NGS",
    "section": "Análisis de datos NGS",
    "text": "Análisis de datos NGS\nControl de Calidad\nAntes de saltar a filtrar tus datos con filtros de calidad que la terminal ejecute muy obediente, lo mejor es ver algunos gráficos básicos que nos dicen mucho más que una serie semi-eterna de caracteres ASCII.\nFASTQC es un programa que hace una serie de análisis básicos y estándar de calidad. La mayoría de las empresas de secuenciación efectúan este análisis y te mandan los resultados junto con tus datos crudos.\nLos análisis de FASTQC son útiles para identificar problemas que pudieron surgir durante el laboratorio o durante la secuenciación.\nEl análisis de FASTQC consiste en los siguientes campos:\n\nBasic Statistics\nPer Base Sequence Quality\nPer Sequence Quality Scores\nPer Base Sequence Content\nPer Sequence GC Content\nPer Base N Content\nSequence Length Distribution\nDuplicate Sequences\nOverrepresented Sequences\nAdapter Content\nKmer Content\nPer Tile Sequence Quality\n\nNotas importantes:\n\nFASTQ automáticamente dice si nuestra muestra “pasó” o “falló” la evaluación. Sin embargo debemos tomar esto dentro del contexto de lo que esperamos de nuestra librería, ya que FASTQ espera una distribución diversa y al azar de nucleótidos, lo que puede no cumplirse en algunos protocolos.\n\nVamos a la página de dicho programa a ver ejemplos de:\n\nBuenos datos Illumina\nMalos datos Illumina\nCorrida Illumina contaminada con dímeros de adaptadores (adapter dimers)\n\n¿Qué que son los dímeros de adaptadores?\nLos adaptadores se ligan al ADN de nuestras muestras en un paso de ligación, sin embargo, también pueden ligarse entre sí y luego pegarse a la flow cell. Resultado: son secuenciados pero no proven datos útiles, simplemente la secuencia de los adaptadores repetida muchas veces. Adelante veremos cómo lidiar con ellos bioinformáticamente, pero se recomienda intentar deshacerse de ellos desde el laboratorio (con pequeños, pequeños imanes como Agencourt o símiles de otras marcas).\n¿Qué tanto importa el análisis FASTQC?\nMucho, a partir del análisis FASTQC es que decidirás si tu secuenciación fue exitosa y qué parámetros de pre-procesamiento deberás utilizar para deshacerte del ruido y quedarte con datos limpios.\nEscoger los parámetros adecuados de pre-procesamiento es vital ya que todas las corridas de secuenciación son diferentes. Además entender bien tu FASTQC puede permitirte rescatar datos usables incluso dentro de una mala corrida.\nComando de Fastqc en Linux\n\nRecuerda moverte a la ubicacion donde se encuentras los archivos Fastq con el comando cd y la direccion.\nmkdir -p results/quality\nfastqc -t 2 *fastq.gz -o results/quality/\nNotas importantes:\n\nDurante el curso veremos que estos comandos pueden aplicarse en R y complementaremos con comandos en Linux.\nFiltrado de Reads\nPre-procesamiento\nEl pre-procesamiento se refiere al filtrado y edición de los datos crudos para quedarnos con los datos limpios, que son los que se analizarán para resolver preguntas biológicas.\nEl input son archivos .fastq y el output son también archivos .fastq (o más posiblemente sus versiones comprimidas).\nEl pre-procesamiento por lo común incluye los siguientes pasos:\nRecortar secuencias por calidad (Sequence Quality Trimming)\nRecortar (quitar) las bases de baja calidad de la secuencia. En Illumina por lo general se trata de las últimas bases (-3’ end). Cuántas bases cortar puede determinarse tras la inspección visual de análisis FASTQC o automáticamente con un parámetro de calidad.\nRecortar secuencias (Trimming)\nRecortar (quitar) x bases de la secuencia porque no nos interesa conservarlas para el análisis (por ejemplo barcodes o sitios de restricción).\nFiltrar secuencias por calidad\nRemueve del conjunto de datos todas las secuencias que estén por debajo de un mínimo de calidad (número de bases con calidad &lt; X, promedio de calidad &lt; X y símiles).\nQuitar adaptadores\nBusca la secuencia de los adaptadores y los recorta de las secuencias finales. También es posible limitar las secuencias finales a sólo aquellas con un adaptador especificado (en vez de otro que pudiera ser contaminación).\nFiltrar artefactos\nDetecta primers de PCRs, quimeras y otros artefactos y los desecha de los datos finales.\nSeparar por barcodes “demultiplexear” (demultiplexing)\nIdentifica las secuencias que contienen uno o más barcodes (también llamado índices), que son secuencias cortas (4-8 bp por lo general) que se incluyen en los adaptadores y que son únicos por muestra. Esto permite identificar y separar todas las secuencias pertenecientes a una muestra de otra secuenciadas al mismo tiempo.\nRequiere que le demos la lista de barcodes utilizados y en qué extremo se localizan. Muchos programas tendrán como output un archivo llamado algo como GATCATGC.fastq.gz, donde se encuentran todas las secuencias del barcode GATCATGC. El nombre de tu muestra deberás ponerlo en un paso subsecuente.\nOjo Tu lista barcodes-nombremuestra es de la info más valiosa de tu proyecto, no la pierdas.\nPaired end merging\nSi se realizó secuenciación Illumina a ambos lados (pair end) es posible unir las lecturas si se detecta que coinciden (aunque sea parcialmente). Esto permite corregir errores de secuenciación al tomar la base de la lectura de mayor calidad.\nRemover otras secuencias no deseadas\nBusca secuencias no deseadas, como genoma de E. coli, restos de PhiX o partes del genoma que no son de interés (e.g. cpDNA).\nComando de Trim_galore en Linux\n\n#trim_galore \ntrim_galore --quality 20 --fastqc --length 250 --output_dir *.gz"
  },
  {
    "objectID": "modulo7.html#análisis-de-conteo-de-k-mers",
    "href": "modulo7.html#análisis-de-conteo-de-k-mers",
    "title": "Fundamentos de NGS",
    "section": "Análisis de conteo de K-mers",
    "text": "Análisis de conteo de K-mers\nEsquema del Tutorial\n\nContar la ocurrencia de k-mers utilizando Jellyfish 2.2.6\nGenerar un histograma utilizando R\nDeterminar la región de copia única y el total de k-mers\nDeterminar la posición del pico y el tamaño del genoma\nComparar la forma del pico con la distribución de Poisson\n\n\n\n\n\n\n\nLectura 1: Conteo de K-meros\n\n\n\n\n\nTamaño de k-mer\nEl tamaño de los k-mers debe ser lo suficientemente grande como para permitir que el k-mer se mapee de manera única en el genoma (un concepto utilizado en el diseño de la longitud de cebadores/oligos para la PCR). K-mers demasiado grandes llevan al uso excesivo de recursos computacionales.\nEn el primer paso, se calcula la frecuencia de k-mers para determinar la cobertura del genoma lograda durante la secuenciación. Existen herramientas de software como Jellyfish que ayudan a encontrar la frecuencia de k-mers en proyectos de secuenciación. La frecuencia de k-mers sigue una distribución pseudo-normal (en realidad, es una distribución de Poisson) alrededor de la cobertura media en el histograma de recuentos de k-mers.\nUna vez que se calculan las frecuencias de k-mers, se traza un histograma para visualizar la distribución y calcular la cobertura media.\n\n\nHistograma de k-mers\n\nLa primera cima (en la región roja) se debe principalmente a errores de secuenciación raros y aleatorios en las lecturas. Los valores en el gráfico pueden recortarse para eliminar lecturas con errores de secuenciación.\n\n\nHistograma de k-mers después del recorte\n\nCon la suposición de que los k-mers se asignan de manera única al genoma, deberían estar presentes solo una vez en una secuencia del genoma. Entonces, su frecuencia reflejará la cobertura del genoma.\nPara fines de cálculo, utilizamos la cobertura media, que es de 14 en el gráfico anterior. El área bajo la curva representará el número total de k-mers.\nEntonces, la estimación del genoma será:\nN = Total de k-mers / Cobertura = Área bajo la curva / cobertura media (14)\nMetodología\nEste estudio tiene como objetivo estimar el tamaño del genoma de una especie utilizando datos de lecturas cortas de baja cobertura. El propósito principal es validar estimaciones previas obtenidas mediante citometría de flujo o generar una nueva estimación computacional.\nLa estimación del tamaño del genoma se basa en el análisis de subsecuencias cortas llamadas k-mers, extraídas de los datos de secuenciación de Illumina. La elección de un tamaño de k-mer adecuado es esencial para la precisión de la estimación.\nEl proceso metodológico incluye las siguientes etapas:\n\nControl de Calidad de las Lecturas: Las lecturas de secuenciación se someten a un riguroso control de calidad utilizando la herramienta Sickle, con un umbral mínimo de calidad Phred de 25. Esto asegura la eliminación de lecturas de baja calidad que pueden afectar la precisión de la estimación.\nCálculo de la Distribución de k-mers: Se utiliza el programa Jellyfish para calcular la distribución de k-mers a partir de las lecturas de secuenciación. Esta distribución es esencial para la estimación del tamaño del genoma.\nConstrucción de un Histograma: Se construye un histograma utilizando el mismo programa Jellyfish para visualizar la distribución de k-mers. Este histograma proporciona información crucial para estimar el tamaño del genoma.\nAnálisis Estadístico con R: Se utiliza el paquete estadístico R para analizar las distribuciones de k-mers. Inicialmente, se traza el conjunto de datos completo, aunque los primeros puntos de datos pueden tener valores atípicos debido a la baja frecuencia. Una vez determinada la posición del pico en el histograma, se calcula el número total de k-mers en la distribución. Esto permite estimar el tamaño del genoma basándose en la posición del pico. Idealmente, la forma del pico debería seguir una distribución de Poisson. Se exploran diferentes tamaños de k-mer para observar una distribución coherente del tamaño del genoma.\n\nEn resumen, este enfoque de estimación del tamaño del genoma combina el análisis de k-mers con el control de calidad de las lecturas y análisis estadísticos para obtener una estimación precisa. Los conjuntos de datos utilizados en el tutorial están disponibles en el clúster BBC, junto con el script utilizado en el proceso.\n\n\n\n\n1. Contar la ocurrencia de k-mers utilizando Jellyfish 2.2.6\nDescargar secuencia de ejemplo Fastq\n\n\nR\nBASH\n\n\n\ndownload.file(\"ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR156/080/SRR15616380/SRR15616380_1.fastq.gz\", \"SRR15616380_1.fastq.gz\")\n\ndownload.file(\"ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR156/080/SRR15616380/SRR15616380_2.fastq.gz\", \"SRR15616380_2.fastq.gz\")\n\n\nwget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR156/080/SRR15616380/SRR15616380_1.fastq.gz\nwget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR156/080/SRR15616380/SRR15616380_2.fastq.gz\n\n\n\nEjecutar jellyfish en Linux\njellyfish count -t 8 -C -m 19 -s 1G -o 19mer_out --min-qual-char=? SRR15616380_1.fastq\n\n\n\n\n\n\nExplicación de parámetros\n\n\n\n\n\nOpciones utilizadas en el conteo de k-mers:\n\n\n-t -treads=unit32 Número de hilos a utilizar en la ejecución, por ejemplo, 1,2,3, etc.\n\n-C -both-strands Contar ambas hebras\n\n-m -mer-len=unit32 Longitud del k-mer\n\n-s -size=unit32 Tamaño del hash/asignación de memoria\n\n-o -output=string Nombre del archivo de salida\n\n--min-quality-char Valor de calidad de base. La versión 2.2.3 de Jellyfish utiliza la puntuación “Phred”, donde “?” = 30.\n\n\n\n\nEsto creará un archivo de salida llamado 19mer_out. Luego, utilizando el archivo 19mer_out creado anteriormente, se crearán puntos de datos para un histograma utilizando el siguiente comando:\njellyfish histo -o 19mer_out.histo 19mer_out\n2. Graficar los resultados utilizando R\nPara visualizar y graficar los datos, utilizamos R. Con el siguiente comando, cargaremos los datos del archivo 19mer_out.histo.\n\ndataframe19 &lt;- read.table(\"19mer_out.histo\") # cargar los datos en dataframe19\n\n\n\n\n\n\n\nErrores Illumina\n\n\n\n\n\nEn general, los k-mers de muy baja frecuencia representan números altos que distorsionarían el eje y. Si observamos los puntos de datos en el archivo de histograma, podemos ver que el primer punto de datos tiene un valor excepcionalmente alto en comparación con el siguiente (segundo) punto de datos. Por lo tanto, eliminamos solo la primera línea y volvemos a trazar utilizando R. A partir de ahora, ignoraremos el primer punto de datos para nuestros cálculos.\n\n\n\n\nplot(dataframe19[2:200,], type=\"l\")\n\n\n\n\n\n\n\n3. Determinar la región de copia única y el número total de k-mers\nEn el examen inicial, el pico comienza desde el segundo punto de datos en adelante. Por lo tanto, ignoramos el primer punto de datos para determinar la región de copia única.\nAhora, utilizando R, volvemos a trazar el gráfico para determinar la región de copia única. Luego incluimos los puntos para mayor claridad en el mismo gráfico.\n\nplot(dataframe19[2:200,], type=\"l\") # traza el gráfico de línea\npoints(dataframe19[2:200,]) # traza los puntos de datos del 2 al 100\n\n\n\n\n\n\n\nSuponiendo que el número total de puntos de datos es 200, ahora podemos calcular el número total de k-mers en la distribución.\n\nsum(as.numeric(dataframe19[2:200,1]*dataframe19[2:200,2]))\n\n[1] 38672486\n\n\n4. Determinar la posición del pico y el tamaño del genoma\nA partir del gráfico trazado, podemos tener una idea de dónde se encuentra la posición del pico mas alto. Usando el siguiente comando, examinaremos los puntos de datos reales entre 40 y 80.\n\ndataframe19[40:80,]\n\n   V1    V2\n40 40  3311\n41 41  4286\n42 42  5200\n43 43  6492\n44 44  7879\n45 45  9416\n46 46 11468\n47 47 13292\n48 48 15395\n49 49 17354\n50 50 18787\n51 51 20985\n52 52 22185\n53 53 23885\n54 54 24896\n55 55 26102\n56 56 27029\n57 57 27132\n58 58 26910\n59 59 26107\n60 60 25867\n61 61 25011\n62 62 23975\n63 63 23021\n64 64 21481\n65 65 20266\n66 66 18585\n67 67 17185\n68 68 15126\n69 69 13399\n70 70 12042\n71 71 10694\n72 72  9157\n73 73  7988\n74 74  7005\n75 75  5772\n76 76  4804\n77 77  3868\n78 78  3414\n79 79  2795\n80 80  2298\n\n\nEn este caso, el pico está en el punto 57. Por lo tanto, el tamaño del genoma se puede estimar como:\nsum(as.numeric(dataframe19[2:200,1]*dataframe19[2:200,2]))/57\n\n[1] 678464.7 ## ~ 678 Mb\nSería interesante ver la proporción de la región de copia única en comparación con el tamaño total del genoma. En este conjunto de datos, la región de copia única se encuentra entre el punto de datos 2 y 100, por lo que el tamaño de la región de copia única se puede calcular aproximadamente como:\nsum(as.numeric(dataframe19[2:100,1]*dataframe19[2:100,2]))/57\n\n[1] 653163.7 ## ~ 653 Mb\nRevisar aqui el genoma de referencia Aqui\nLa proporción se puede calcular como:\n\n(sum(as.numeric(dataframe19[2:100,1]*dataframe19[2:100,2]))) / (sum(as.numeric(dataframe19[2:200,1]*dataframe19[2:200,2])))\n\n[1] 0.9627086\n\n\n5. Comparar la forma del pico con la distribución de Poisson\nDado que tenemos una curva agradable, podemos comparar nuestra curva con una curva teórica, que es la distribución de Poisson.\n\nsingleC &lt;- sum(as.numeric(dataframe19[2:200,1]*dataframe19[2:200,2]))/57\npoisdtb &lt;- dpois(1:100,57)*singleC\nplot(poisdtb, type='l', lty=2, col=\"green\")\nlines(dataframe19[1:100,57] * singleC, type = \"l\", col=3)\nlines(dataframe19[1:100,],type= \"l\")",
    "crumbs": [
      "Fundamentos NGS",
      "Fundamentos de NGS"
    ]
  },
  {
    "objectID": "modulo7.html#alineamiento",
    "href": "modulo7.html#alineamiento",
    "title": "Fundamentos de NGS",
    "section": "Alineamiento",
    "text": "Alineamiento\n\nFormato SAM/BAM\nEl formato SAM (Sequence Alignment Map) y su versión binaria comprimida BAM (Binary Alignment Map) son herramientas cruciales en el análisis de datos de secuenciación de nueva generación (NGS). Estos formatos se utilizan para representar alineamientos de secuencias con el objetivo de mapear las letras (bases) de dos o más secuencias, permitiendo incluso la detección de espaciadores (indels) y otros tipos de variaciones. A continuación, se detalla el contenido y la importancia de estos formatos:\n\n\n\n\n\n\nContenido del formato SAM\n\n\n\n\n\nEl formato SAM consta de dos partes principales: el encabezado (Header) y el cuerpo del alineamiento (Alignment). El encabezado contiene líneas que comienzan con “@” y proporciona información vital sobre el alineamiento, como la longitud de cada cromosoma, el programa utilizado para generar el alineamiento, entre otros detalles relevantes.\nEl cuerpo del alineamiento incluye una línea por cada alineamiento de secuencias. Cada línea contiene varias columnas que describen las características del alineamiento. Aquí se enumeran las columnas y su significado:\nRead id: Identificador único de la lectura.\nFLAG: Información sobre el alineamiento, como orientación, calidad, etc.\nChr: Cromosoma al que se alinea la secuencia.\nStart: Posición de inicio en el cromosoma.\nMapping quality: Calidad del mapeo.\nCIGAR (alignment): Describe las operaciones de alineamiento, como inserciones, deleciones y sustituciones.\nMateChr: Cromosoma del compañero de lectura (en el caso de lecturas pareadas).\nMateStart: Posición de inicio del compañero de lectura.\nMateDist: Distancia entre las lecturas pareadas.\nQuerySeq: Secuencia de la lectura.\nQueryBaseQuals: Calidad de las bases en la lectura.\nAlignmentScore: Puntuación de alineamiento.\nEdit-distance-to-reference: Distancia de edición con respecto a la secuencia de referencia.\nNumber-of-hits: Número de alineamientos similares.\nStrand: Orientación de la lectura.\nHit-index: Índice del alineamiento.\n\n\n\nEjemplo de un alineamiento en SAM:\nAlineamiento convecional\nCoor     12345678901234  5678901234567890123456789012345\nref      AGCATGTTAGATAA**GATAGCTGTGCTAGTAGGCAGTCAGCGCCAT\n+r001/1        TTAGATAAAGGATA*CTG\n+r002         aaaAGATAA*GGATA\n+r003       gcctaAGCTAA\n+r004                     ATAGCT..............TCAGC\n-r003                            ttagctTAGGC\n-r001/2                                        CAGCGGCAT\nEn SAM se codifica así:\n@HD VN:1.5 SO:coordinate\n@SQ SN:ref LN:45\nr001   99 ref  7 30 8M2I4M1D3M = 37  39 TTAGATAAAGGATACTG *\nr002    0 ref  9 30 3S6M1P1I4M *  0   0 AAAAGATAAGGATA    *\nr003    0 ref  9 30 5S6M       *  0   0 GCCTAAGCTAA       * SA:Z:ref,29,-,6H5M,17,0;\nr004    0 ref 16 30 6M14N5M    *  0   0 ATAGCTTCAGC       *\nr003 2064 ref 29 17 6H5M       *  0   0 TAGGC             * SA:Z:ref,9,+,5S6M,30,1;\nr001  147 ref 37 30 9M         =  7 -39 CAGCGGCAT         * NM:i:1\n\n\nPreparacion del index\nAdemás de la representación de alineamientos, también es importante entender cómo se prepara el índice del genoma de referencia y cómo se realizan los alineamientos de secuencias. El índice se crea a partir del genoma de referencia y es esencial para realizar alineamientos eficientes. Aquí se presentan los comandos utilizados en el proceso:\n\nBASHR\n\n\n## creamos una carpeta para el indexado de genoma\n\nmkdir index \ncd index\n\n## descarga de genoma de referencia\n\nwget https://ftp.ncbi.nlm.nih.gov/genomes/refseq/bacteria/Metamycoplasma_hominis/latest_assembly_versions/GCF_000085865.1_ASM8586v1/GCF_000085865.1_ASM8586v1_genomic.fna.gz\n\n## realizamos el indexado del genoma\n\nbwa index -p ref GCF_000085865.1_ASM8586v1_genomic.fna.gz\n\n\n## Descargar genoma\n\nurl &lt;- fromJSON(file = \"data/reference/ref.json\")\nfilename &lt;- \"GCF_000085865.1_ASM8586v1_genomic.fna.gz\"\ndownload_genome(url$urlref, filename, \"data/reference/\")\n\n## Generar Indice\nbowtie2_build(\"data/reference/GCF_000085865.1_ASM8586v1_genomic.fna\", \n    bt2Index = \"data/reference/index/myco\" , overwrite = TRUE)\n\n\n\n\n\n\n\n\n\nAclaracion\n\n\n\nEn esta sección se plantea la ejecución en paralelo de R y BASH. Despues ahondaremos en R.\n\n\n\n\nAlineamiento de secuencias\nEstos comandos ilustran la descarga del genoma de referencia, la creación de un índice para el genoma y el proceso de alineamiento de secuencias utilizando la herramienta Bowtie2. Posteriormente, los alineamientos se convierten en formato BAM utilizando Samtools.\n\nBASHR\n\n\n## Alineamineto de genoma\nbwa mem -t 4 -o gen.sam reference/index/ref SRR15616380_1.fastq.gz SRR15616380_2.fastq.gz\n\n## Creación de Archivo BAM\nsamtools view -u@ 2 gen.sam | \\\nsamtools sort -@ 2 -o gen.sorted.bam -\nsamtools index gen.sorted.bam\n\n\n## Alineamineto de genoma\nbowtie2(bt2Index = \"data/reference/index/myco\", \n        samOutput = \"results/SRR15616379.sam\", \n        seq1 = \"data/processed_data/filtered_F/SRR15616379_filt_1.fastq\", \n        seq2 = \"data/processed_data/filtered_R/SRR15616379_filt_2.fastq\", \n        \"--threads=3\")\n        \n## Creación de Archivo BAM\nasBam(\"results/SRR15616379.sam\")       \n\n\n\n\n\n\n\n\n\nEn resumen\n\n\n\nEl formato SAM/BAM y los procesos de indexado y alineamiento son elementos esenciales en el análisis de datos de secuenciación de nueva generación (NGS), permitiendo la representación y procesamiento de alineamientos de secuencias de manera eficiente. Para obtener más información detallada sobre estos formatos y sus aplicaciones, se recomienda consultar la documentación oficial de Samtools y la especificación del formato SAM aquí.",
    "crumbs": [
      "Fundamentos NGS",
      "Fundamentos de NGS"
    ]
  },
  {
    "objectID": "modulo8.html",
    "href": "modulo8.html",
    "title": "Genómica con R",
    "section": "",
    "text": "En este tutorial, aprenderemos a configurar un entorno de trabajo en R para realizar análisis de bioinformática genómica. Cubriremos la instalación de las bibliotecas necesarias y cómo cargarlas en R. A lo largo del tutorial, utilizaremos el código proporcionado en el archivo main.R.",
    "crumbs": [
      "Fundamentos NGS",
      "Genómica con R"
    ]
  },
  {
    "objectID": "modulo8.html#introducción",
    "href": "modulo8.html#introducción",
    "title": "Genómica con R",
    "section": "",
    "text": "En este tutorial, aprenderemos a configurar un entorno de trabajo en R para realizar análisis de bioinformática genómica. Cubriremos la instalación de las bibliotecas necesarias y cómo cargarlas en R. A lo largo del tutorial, utilizaremos el código proporcionado en el archivo main.R.",
    "crumbs": [
      "Fundamentos NGS",
      "Genómica con R"
    ]
  },
  {
    "objectID": "modulo8.html#instalación-y-carga-de-bibliotecas",
    "href": "modulo8.html#instalación-y-carga-de-bibliotecas",
    "title": "Genómica con R",
    "section": "Instalación y carga de bibliotecas",
    "text": "Instalación y carga de bibliotecas\nComenzamos instalando y cargando las bibliotecas requeridas. Asegúrate de que R esté configurado para instalar paquetes si aún no tienes estas bibliotecas instaladas. Ejecuta el siguiente código en tu consola de R:\nsource(\"install.R\", local = TRUE)\n\n\n\n\n\n\ninstall.R\n\n\n\n\n\nEl archivo install.R contiene un script para instalar las bibliotecas necesarias. A continuación, se muestra el contenido de install.R:\n# Package R dependencies\nif (!is.element(\"devtools\", rownames(installed.packages()))) {\n  install.packages(\"devtools\")\n  install.packages(\"BiocManager\")\n}\n\nlibrary(devtools)\n\n# Install missing packages\nmissingPackages &lt;- function(pkg) {\n  if (!is.element(pkg, rownames(installed.packages()))) {\n    message(pkg, \"-----&gt; Package is not installed \")\n    BiocManager::install(pkg)\n  }\n}\n\ndependencies &lt;- c(\n  \"dada2\", \"Rbowtie2\", \"Rsamtools\", \"ape\", \"rjson\",\n  \"dplyr\", \"foreach\", \"doParallel\", \"tidyr\", \"Biostrings\",\n  \"gmapR\", \"fastqcr\", \"ShortRead\"\n)\n\nfor (i in dependencies) {\n  missingPackages(i)\n  library(i, character.only = TRUE)\n}\n\nif (!is.element(\"GEOquery\", rownames(installed.packages()))) {\n  devtools::install_github('GEOquery', 'seandavi')\n}\n\nif (!is.element(\"radiator\", rownames(installed.packages()))) {\n  devtools::install_github(\"thierrygosselin/radiator\")\n}\n\nlibrary(radiator)",
    "crumbs": [
      "Fundamentos NGS",
      "Genómica con R"
    ]
  },
  {
    "objectID": "modulo8.html#descarga-y-procesamiento-de-datos",
    "href": "modulo8.html#descarga-y-procesamiento-de-datos",
    "title": "Genómica con R",
    "section": "Descarga y procesamiento de datos",
    "text": "Descarga y procesamiento de datos\nEn esta sección, exploraremos el proceso de descarga y procesamiento de datos utilizando el código proporcionado en main.R.\nDescarga de herramientas y bibliotecas\nPrimero, asegurémonos de que las herramientas y bibliotecas necesarias estén instaladas. Esto incluye el conjunto de herramientas SRA Toolkit y las bibliotecas devtools, GEOquery, y dplyr.\nsource(\"scripts/sratoolkit.R\", local = TRUE)\nsource(\"scripts/listfastq.R\", local = TRUE)\nsource(\"scripts/filtereads.R\", local = TRUE)\nsource(\"scripts/consensus.R\", local = TRUE)\nlibrary(devtools)\nlibrary(GEOquery)\nlibrary(dplyr)\nDescarga de SRA Toolkit\nEl siguiente código descarga SRA Toolkit dependiendo del sistema operativo (Windows o Linux) y configura las funciones necesarias para su uso.\n# Código para descargar SRA Toolkit y configurar funciones específicas\n# según el sistema operativo (Windows o Linux)\n# ...\n\n# Funciones para descargar y dividir archivos SRA\nsetClass(\"sratoolkit\", slots = list(dest = \"character\", sraid = \"character\"))\n\nsetGeneric(\"downloadsra\", function(obj) {\n  standardGeneric(\"downloadsra\")\n})\n\nsetGeneric(\"fastqdump\", function(obj, output) {\n  standardGeneric(\"fastqdump\")\n})\nDescarga de datos SRA y conversión a archivos FASTQ\nA continuación, se muestra un ejemplo de cómo descargar archivos SRA y convertirlos en archivos FASTQ utilizando las funciones configuradas anteriormente.\n# Función para descargar archivos SRA y convertirlos en archivos FASTQ\ndownload_sra_files &lt;- function(id, output = paste0(getwd(), \"/data/rawdata/\")) {\n  tmp &lt;- new(\"sratoolkit\", dest = output, sraid = id)\n  downloadsra(tmp)\n  fastqdump(tmp, output)\n}\n\n# Ejemplo de uso\ndownload_sra_files(\"SRA_ID\", \"output_directory/\")\nEsta función download_sra_files toma un identificador de SRA y un directorio de salida como argumentos y descarga los archivos SRA correspondientes, luego los convierte en archivos FASTQ en el directorio de salida especificado.\nPuedes personalizar esta función según tus necesidades específicas en tu curso de bioinformática genómica.\nListado de archivos FASTQ\nEl archivo listfastq.R contiene una función llamada list_fastq que se utiliza para listar los archivos FASTQ en un directorio específico y organizarlos según un patrón.\n# Código para listar archivos FASTQ y organizarlos por patrón\n# \nEjemplo de uso de la función fastq_list &lt;- list_fastq(“directorio/”)\nLa función list_fastq toma dos argumentos opcionales: ruta (ruta del directorio) y pattern (patrón de nombres de archivo). Esta función lista los archivos FASTQ en el directorio especificado y organiza las lecturas según el patrón dado.\nFiltrado y recorte de lecturas\nEl archivo filterreads.R contiene una función llamada filter_reads que se utiliza para filtrar y recortar lecturas de archivos FASTQ.\nEjemplo de uso de la función\nlog &lt;- filter_reads(\"nombre_muestra\", \"archivo_lectura_F\", \"archivo_lectura_R\", trunc = 150)\nLa función filter_reads toma varios argumentos, incluyendo el nombre de la muestra, los archivos de lectura hacia adelante y hacia atrás, y parámetros de filtrado y recorte. Esta función devuelve un registro de las secuencias recuperadas después del filtrado.\nGeneración de consenso\nEl archivo consensus.R contiene una función llamada consensus_parallel que se utiliza para generar secuencias de consenso a partir de datos de secuenciación.\nLa función consensus_parallel toma varios argumentos, incluyendo el rango de posiciones, profundidad, umbral de frecuencia y otros. Esta función genera secuencias de consenso a partir de los datos y las devuelve como una cadena."
  },
  {
    "objectID": "modulo8.html#descarga-de-genomas-desde-ncbi",
    "href": "modulo8.html#descarga-de-genomas-desde-ncbi",
    "title": "Genómica con R",
    "section": "Descarga de genomas desde NCBI",
    "text": "Descarga de genomas desde NCBI\nEn esta sección, se realiza la descarga de un genoma desde la base de datos del NCBI utilizando el código proporcionado:\nurl &lt;- fromJSON(file = \"data/reference/ref.json\")\nfilename &lt;- \"GCF_000085865.1_ASM8586v1_genomic.fna.gz\"\ndownload_genome(url$urlref, filename, \"data/reference/\")\n::: {callout-warning title=“Aclaración”}\nEl código utiliza el archivo JSON ref.json para obtener la URL del genoma y luego descarga el genoma en formato FASTA comprimido. Los archivos de genoma descargados se almacenan en el directorio data/reference/.\n:::"
  },
  {
    "objectID": "modulo8.html#descarga-de-archivos-desde-sra---ncbi",
    "href": "modulo8.html#descarga-de-archivos-desde-sra---ncbi",
    "title": "Genómica con R",
    "section": "Descarga de archivos desde SRA - NCBI",
    "text": "Descarga de archivos desde SRA - NCBI\nEsta sección demuestra cómo descargar archivos desde la Base de Datos de Archivos de Secuencia (SRA) de NCBI utilizando la función download_sra_files:\ndownload_sra_files(id = \"SRR15616379\")\ndownload_sra_files(id = \"SRR26204001\")\n::: {callout-tip title=“En resumen”}\nEl código descarga archivos SRA utilizando los identificadores específicos proporcionados (SRR15616379 y SRR26204001) y luego los convierte en archivos FASTQ en el directorio especificado. Ver Aqui\n:::"
  },
  {
    "objectID": "modulo8.html#control-de-calidad-de-archivos-fastq",
    "href": "modulo8.html#control-de-calidad-de-archivos-fastq",
    "title": "Genómica con R",
    "section": "Control de calidad de archivos FASTQ",
    "text": "Control de calidad de archivos FASTQ\nEn esta sección, se realiza un control de calidad de los archivos FASTQ descargados utilizando la biblioteca fastqc. Primero, se listan las lecturas FASTQ y se genera un perfil de calidad:\nlecturas &lt;- list_fastq(pattern = c(\"SRR15616379_1.fastq.gz\", \"SRR15616379_2.fastq.gz\"))\nLuego, se ejecuta fastqc para realizar un control de calidad más detallado:\n\nlibrary(fastqcr)\nfastqc(fq.dir = \"data/rawdata/\", # Directorio de archivos FASTQ\n       qc.dir = \"results/\", # Directorio de resultados\n       threads = 4,\n       fastqc.path = \"/home/fascue/Descargas/FastQC/fastqc\"\n)\n\nEste código genera informes de calidad en el directorio results/."
  },
  {
    "objectID": "modulo8.html#informe-de-calidad-multiqc",
    "href": "modulo8.html#informe-de-calidad-multiqc",
    "title": "Genómica con R",
    "section": "Informe de calidad multiqc",
    "text": "Informe de calidad multiqc\nPara generar un informe de calidad consolidado utilizando MultiQC, se utiliza el siguiente código:\nqc_report(qc.dir, result.file = \"results/multiqc/\",\n          experiment = \"Mycoplasma\")\n::: {callout-tip title=“Expliación” collapse=“true”}\nEste código genera un informe de calidad consolidado en el directorio results/multiqc/ utilizando la información de calidad previamente generada por FastQC.\nAdemás, se muestran ejemplos de cómo realizar varios tipos de análisis y visualizaciones de calidad utilizando la biblioteca qc_report. Estos ejemplos incluyen el análisis de contenido de GC, calidad de secuencia base por base, duplicación de secuencias y más.\n:::\nPlots de calidad FastQC\n::: {callout-tip title=“Per base sequence quality” collapse=“true”}\n\nqc &lt;- qc_read(\"results/SRR15616379_1_fastqc.zip\")\n\nReading: results/SRR15616379_1_fastqc.zip\n\n\nRows: 10 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (3): status, module, sample\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 8 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (2): Measure, Value\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 58 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (1): Base\ndbl (6): Mean, Median, Lower Quartile, Upper Quartile, 10th Percentile, 90th...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 13 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\ndbl (2): Quality, Count\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 58 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (1): Base\ndbl (4): G, A, T, C\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 101 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\ndbl (2): GC Content, Count\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 58 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (1): Base\ndbl (1): N-Count\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 41 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (1): Length\ndbl (1): Count\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 16 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (1): Duplication Level\ndbl (1): Percentage of total\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 0 Columns: 0\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 56 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (1): Position\ndbl (6): Illumina Universal Adapter, Illumina Small RNA 3' Adapter, Illumina...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 0 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (2): Total Deduplicated Percentage, 85.14753304887951\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nqc_plot(qc, \"Per base sequence quality\")\n\n\n\n\n:::\n::: {callout-tip title=“Per sequence quality scores” collapse=“true”}\n\nqc_plot(qc, \"Per sequence quality scores\")\n\n\n\n\n:::\n::: {callout-tip title=“Per base sequence content” collapse=“true”}\n\nqc_plot(qc, \"Per base sequence content\")\n\n\n\n\n:::\n::: {callout-tip title=“Per sequence GC content” collapse=“true”}\n\nqc_plot(qc, \"Per sequence GC content\")\n\n\n\n\n:::\n::: {callout-tip title=“Sequence duplication levels” collapse=“true”}\n\nqc_plot(qc, \"Sequence duplication levels\")\n\n\n\n\n:::\n::: {callout-tip title=“Overrepresented sequences” collapse=“true”}\n\nqc_plot(qc, \"Overrepresented sequences\")\n\n\n\n\n:::\n::: {callout-tip title=“Adapter content” collapse=“true”}\n\nqc_plot(qc, \"Adapter content\")\n\n\n\n\n:::"
  },
  {
    "objectID": "modulo8.html#filtrado-de-lecturas-de-archivos-fastq",
    "href": "modulo8.html#filtrado-de-lecturas-de-archivos-fastq",
    "title": "Genómica con R",
    "section": "Filtrado de lecturas de archivos FASTQ",
    "text": "Filtrado de lecturas de archivos FASTQ\nEn esta sección, se realiza el filtrado de lecturas de los archivos FASTQ utilizando la función filter_reads previamente definida:\nlog_filter &lt;- filter_reads(name = lecturas$name, lf = lecturas$lf, \n             lr = lecturas$lr, trunc = 250)\nEl código ejecuta la función filter_reads en las lecturas previamente listadas, aplicando un valor de truncamiento de 250. El registro del filtrado se almacena en la variable log_filter."
  },
  {
    "objectID": "modulo8.html#ensamblaje-de-genomas",
    "href": "modulo8.html#ensamblaje-de-genomas",
    "title": "Genómica con R",
    "section": "Ensamblaje de genomas",
    "text": "Ensamblaje de genomas\nEn esta sección, se realiza el ensamblaje de genomas utilizando la biblioteca bowtie2. Primero, se descomprime el genoma de referencia:\ngunzip(\"data/reference/GCF_000085865.1_ASM8586v1_genomic.fna.gz\")\nLuego, se construye el índice de referencia para bowtie2:\nbowtie2_build(\"data/reference/GCF_000085865.1_ASM8586v1_genomic.fna\",\n              bt2Index = \"data/reference/index/myco\" , overwrite = TRUE)\nFinalmente, se realiza el alineamiento de los archivos FASTQ a la referencia utilizando bowtie2:\nbowtie2(bt2Index = \"data/reference/index/myco\", \n        samOutput = \"results/SRR15616379.sam\", \n        seq1 = \"data/processed_data/filtered_F/SRR15616379_filt_1.fastq\", \n        seq2 = \"data/processed_data/filtered_R/SRR15616379_filt_2.fastq\", \n        \"--threads=3\")\n\n\n\n\n\n\nExplicación de proceso\n\n\n\nEstos pasos incluyen la construcción del índice de referencia y el alineamiento de las lecturas de secuenciación en los archivos FASTQ a la referencia genómica. El resultado se almacena en un archivo SAM en el directorio results/."
  },
  {
    "objectID": "modulo8.html#manipulación-de-archivos-de-alineación",
    "href": "modulo8.html#manipulación-de-archivos-de-alineación",
    "title": "Genómica con R",
    "section": "Manipulación de archivos de alineación",
    "text": "Manipulación de archivos de alineación\nEn esta sección, se realizan diversas operaciones en archivos de alineación BAM utilizando la biblioteca Rsamtools.\nConversión a archivos BAM\nPrimero, se convierte el archivo SAM previamente generado en un archivo BAM:\nasBam(\"results/SRR15616379.sam\")\nLectura de archivo BAM\nSe carga el archivo BAM para realizar estadísticas de alineación:\n\nlibrary(Rsamtools)\n\nLoading required package: GenomeInfoDb\n\n\nLoading required package: BiocGenerics\n\n\n\nAttaching package: 'BiocGenerics'\n\n\nThe following objects are masked from 'package:stats':\n\n    IQR, mad, sd, var, xtabs\n\n\nThe following objects are masked from 'package:base':\n\n    anyDuplicated, append, as.data.frame, basename, cbind, colnames,\n    dirname, do.call, duplicated, eval, evalq, Filter, Find, get, grep,\n    grepl, intersect, is.unsorted, lapply, Map, mapply, match, mget,\n    order, paste, pmax, pmax.int, pmin, pmin.int, Position, rank,\n    rbind, Reduce, rownames, sapply, setdiff, sort, table, tapply,\n    union, unique, unsplit, which.max, which.min\n\n\nLoading required package: S4Vectors\n\n\nLoading required package: stats4\n\n\n\nAttaching package: 'S4Vectors'\n\n\nThe following objects are masked from 'package:base':\n\n    expand.grid, I, unname\n\n\nLoading required package: IRanges\n\n\nLoading required package: GenomicRanges\n\n\nLoading required package: Biostrings\n\n\nLoading required package: XVector\n\n\n\nAttaching package: 'Biostrings'\n\n\nThe following object is masked from 'package:base':\n\n    strsplit\n\nbamFile &lt;- BamFile(\"results/SRR15616379.bam\")\n\nEstadísticas de alineación\nSe calculan estadísticas de alineación a partir del archivo BAM:\n\nbam &lt;- countBam(bamFile)\nquickBamFlagSummary(bamFile)\n\n                                group |    nb of |    nb of | mean / max\n                                   of |  records |   unique | records per\n                              records | in group |   QNAMEs | unique QNAME\nAll records........................ A |   377410 |   188705 |    2 / 2\n  o template has single segment.... S |        0 |        0 |   NA / NA\n  o template has multiple segments. M |   377410 |   188705 |    2 / 2\n      - first segment.............. F |   188705 |   188705 |    1 / 1\n      - last segment............... L |   188705 |   188705 |    1 / 1\n      - other segment.............. O |        0 |        0 |   NA / NA\n\nNote that (S, M) is a partitioning of A, and (F, L, O) is a partitioning of M.\nIndentation reflects this.\n\nDetails for group M:\n  o record is mapped.............. M1 |   322599 |   167765 | 1.92 / 2\n      - primary alignment......... M2 |   322599 |   167765 | 1.92 / 2\n      - secondary alignment....... M3 |        0 |        0 |   NA / NA\n  o record is unmapped............ M4 |    54811 |    33871 | 1.62 / 2\n\nDetails for group F:\n  o record is mapped.............. F1 |   166780 |   166780 |    1 / 1\n      - primary alignment......... F2 |   166780 |   166780 |    1 / 1\n      - secondary alignment....... F3 |        0 |        0 |   NA / NA\n  o record is unmapped............ F4 |    21925 |    21925 |    1 / 1\n\nDetails for group L:\n  o record is mapped.............. L1 |   155819 |   155819 |    1 / 1\n      - primary alignment......... L2 |   155819 |   155819 |    1 / 1\n      - secondary alignment....... L3 |        0 |        0 |   NA / NA\n  o record is unmapped............ L4 |    32886 |    32886 |    1 / 1\n\nseqinfo(bamFile)\n\nSeqinfo object with 1 sequence from an unspecified genome:\n  seqnames    seqlengths isCircular genome\n  NC_013511.1     665445         NA   &lt;NA&gt;\n\n\nConteo de profundidad por posición\n\n# count position of alignment \nres &lt;- pileup(bamFile)\n\nhead(res)\n\n     seqnames pos strand nucleotide count\n1 NC_013511.1   1      +          A     1\n2 NC_013511.1   1      +          C     1\n3 NC_013511.1   1      -          C     1\n4 NC_013511.1   1      +          G     1\n5 NC_013511.1   1      +          T     6\n6 NC_013511.1   1      -          T     9\n\ntable(res$strand, res$nucleotide)\n\n   \n         A      C      G      T      N      =      -      +\n  + 259168  97674 131968 236724      0      0   4042      0\n  - 239533 127380  97801 256096      0      0   3281      0\n  *      0      0      0      0      0      0      0      0\n\n# coverage plot\n\ncover &lt;- res[,c(\"pos\",\"count\")]\n\n#plot(count ~ pos, cover , pch =\".\")\n\nPlot de profundidad\n\nlibrary(ggplot2)\n# Crear un gráfico de cobertura utilizando ggplot2\nggplot(cover, aes(x = pos, y = count)) +\n  geom_point(shape = \".\", size = 1) +\n  labs(title = \"Gráfico de Cobertura\",\n       x = \"Posición\",\n       y = \"Cobertura\") +\n  theme_minimal()\n\n\n\n\nManipulación de archivos BAM de gran tamaño\nSe muestra cómo manejar archivos BAM de gran tamaño ajustando el tamaño de lectura:\nyieldSize(bamFile) &lt;- 1\nopen(bamFile)\nscanBam(bamFile)[[1]]$seq\nclose(bamFile)\nyieldSize(bamFile) &lt;- NA"
  },
  {
    "objectID": "data/prokka/CODE_OF_CONDUCT.html",
    "href": "data/prokka/CODE_OF_CONDUCT.html",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.\n\n\n\nExamples of behavior that contributes to creating a positive environment include:\n\nUsing welcoming and inclusive language\nBeing respectful of differing viewpoints and experiences\nGracefully accepting constructive criticism\nFocusing on what is best for the community\nShowing empathy towards other community members\n\nExamples of unacceptable behavior by participants include:\n\nThe use of sexualized language or imagery and unwelcome sexual attention or advances\nTrolling, insulting/derogatory comments, and personal or political attacks\nPublic or private harassment\nPublishing others’ private information, such as a physical or electronic address, without explicit permission\nOther conduct which could reasonably be considered inappropriate in a professional setting\n\n\n\n\nProject maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior.\nProject maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.\n\n\n\nThis Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.\n\n\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at torsten.seemann+coc@gmail.com. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately.\nProject maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project’s leadership.\n\n\n\nThis Code of Conduct is adapted from the Contributor Covenant, version 1.4, available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html\nFor answers to common questions about this code of conduct, see https://www.contributor-covenant.org/faq"
  },
  {
    "objectID": "data/prokka/CODE_OF_CONDUCT.html#our-pledge",
    "href": "data/prokka/CODE_OF_CONDUCT.html#our-pledge",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation."
  },
  {
    "objectID": "data/prokka/CODE_OF_CONDUCT.html#our-standards",
    "href": "data/prokka/CODE_OF_CONDUCT.html#our-standards",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "Examples of behavior that contributes to creating a positive environment include:\n\nUsing welcoming and inclusive language\nBeing respectful of differing viewpoints and experiences\nGracefully accepting constructive criticism\nFocusing on what is best for the community\nShowing empathy towards other community members\n\nExamples of unacceptable behavior by participants include:\n\nThe use of sexualized language or imagery and unwelcome sexual attention or advances\nTrolling, insulting/derogatory comments, and personal or political attacks\nPublic or private harassment\nPublishing others’ private information, such as a physical or electronic address, without explicit permission\nOther conduct which could reasonably be considered inappropriate in a professional setting"
  },
  {
    "objectID": "data/prokka/CODE_OF_CONDUCT.html#our-responsibilities",
    "href": "data/prokka/CODE_OF_CONDUCT.html#our-responsibilities",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior.\nProject maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful."
  },
  {
    "objectID": "data/prokka/CODE_OF_CONDUCT.html#scope",
    "href": "data/prokka/CODE_OF_CONDUCT.html#scope",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers."
  },
  {
    "objectID": "data/prokka/CODE_OF_CONDUCT.html#enforcement",
    "href": "data/prokka/CODE_OF_CONDUCT.html#enforcement",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at torsten.seemann+coc@gmail.com. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately.\nProject maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project’s leadership."
  },
  {
    "objectID": "data/prokka/CODE_OF_CONDUCT.html#attribution",
    "href": "data/prokka/CODE_OF_CONDUCT.html#attribution",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "This Code of Conduct is adapted from the Contributor Covenant, version 1.4, available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html\nFor answers to common questions about this code of conduct, see https://www.contributor-covenant.org/faq"
  },
  {
    "objectID": "modulo9.html",
    "href": "modulo9.html",
    "title": "Consenso y Anotación",
    "section": "",
    "text": "Lectura 2: Consenso de Genoma\n\n\n\n\n\nEl consenso de genoma es un proceso fundamental en la bioinformática genómica que implica la generación de una secuencia de ADN representativa a partir de múltiples secuencias de lecturas de ADN. En los proyectos de secuenciación masiva de próxima generación (NGS), las secuencias de lecturas provienen de fragmentos de ADN secuenciados y suelen ser cortas y fragmentadas. El objetivo del consenso de genoma es reconstruir una secuencia de genoma completa y precisa a partir de estas lecturas.\n\nEl consenso de genoma es esencial para una variedad de aplicaciones en genómica y biología molecular. Algunos de los usos más comunes incluyen:\n\nEnsamblaje de Genomas: En proyectos de secuenciación de novo, donde no se dispone de una referencia genómica, el consenso de genoma se utiliza para ensamblar el genoma completo a partir de las secuencias de lecturas.\nVariación Genómica: La identificación de variantes genómicas, como mutaciones o polimorfismos, se basa en comparaciones entre el genoma de referencia y las lecturas secuenciadas.\nAnálisis de Metagenómica: En estudios de metagenómica, donde se analiza el ADN de múltiples organismos en una muestra, el consenso de genoma permite identificar y caracterizar las especies presentes.\nEstudios de Filogenia: La construcción de árboles filogenéticos y la comparación de genomas entre especies se basan en secuencias de consenso.\n\nEl proceso de consenso de genoma implica la alineación y comparación de las secuencias de lecturas contra un genoma de referencia o entre ellas mismas. A medida que se realizan estas comparaciones, se determina la secuencia de consenso, que representa la secuencia de ADN más probable en las regiones cubiertas por las lecturas.\n\nPara realizar el consenso de genoma, se utilizan herramientas y bibliotecas bioinformáticas especializadas, como Rsamtools, Samtools, Bioconductor y otras, que permiten el procesamiento eficiente de archivos de alineación (BAM/SAM) y la generación de secuencias de consenso de alta calidad.\nEl consenso de genoma es una parte esencial de la investigación en genómica y juega un papel crucial en la comprensión de la estructura y la función de los genomas. A medida que avanza la tecnología de secuenciación, las técnicas y las herramientas de consenso de genoma continúan evolucionando para abordar desafíos cada vez más complejos en la genómica.",
    "crumbs": [
      "Genomica y Metagenomica",
      "Consenso y Anotación"
    ]
  },
  {
    "objectID": "modulo9.html#introducción-al-consenso-de-genoma",
    "href": "modulo9.html#introducción-al-consenso-de-genoma",
    "title": "Consenso y Anotación",
    "section": "",
    "text": "Lectura 2: Consenso de Genoma\n\n\n\n\n\nEl consenso de genoma es un proceso fundamental en la bioinformática genómica que implica la generación de una secuencia de ADN representativa a partir de múltiples secuencias de lecturas de ADN. En los proyectos de secuenciación masiva de próxima generación (NGS), las secuencias de lecturas provienen de fragmentos de ADN secuenciados y suelen ser cortas y fragmentadas. El objetivo del consenso de genoma es reconstruir una secuencia de genoma completa y precisa a partir de estas lecturas.\n\nEl consenso de genoma es esencial para una variedad de aplicaciones en genómica y biología molecular. Algunos de los usos más comunes incluyen:\n\nEnsamblaje de Genomas: En proyectos de secuenciación de novo, donde no se dispone de una referencia genómica, el consenso de genoma se utiliza para ensamblar el genoma completo a partir de las secuencias de lecturas.\nVariación Genómica: La identificación de variantes genómicas, como mutaciones o polimorfismos, se basa en comparaciones entre el genoma de referencia y las lecturas secuenciadas.\nAnálisis de Metagenómica: En estudios de metagenómica, donde se analiza el ADN de múltiples organismos en una muestra, el consenso de genoma permite identificar y caracterizar las especies presentes.\nEstudios de Filogenia: La construcción de árboles filogenéticos y la comparación de genomas entre especies se basan en secuencias de consenso.\n\nEl proceso de consenso de genoma implica la alineación y comparación de las secuencias de lecturas contra un genoma de referencia o entre ellas mismas. A medida que se realizan estas comparaciones, se determina la secuencia de consenso, que representa la secuencia de ADN más probable en las regiones cubiertas por las lecturas.\n\nPara realizar el consenso de genoma, se utilizan herramientas y bibliotecas bioinformáticas especializadas, como Rsamtools, Samtools, Bioconductor y otras, que permiten el procesamiento eficiente de archivos de alineación (BAM/SAM) y la generación de secuencias de consenso de alta calidad.\nEl consenso de genoma es una parte esencial de la investigación en genómica y juega un papel crucial en la comprensión de la estructura y la función de los genomas. A medida que avanza la tecnología de secuenciación, las técnicas y las herramientas de consenso de genoma continúan evolucionando para abordar desafíos cada vez más complejos en la genómica.",
    "crumbs": [
      "Genomica y Metagenomica",
      "Consenso y Anotación"
    ]
  },
  {
    "objectID": "modulo9.html#importancia-del-consenso-de-genoma",
    "href": "modulo9.html#importancia-del-consenso-de-genoma",
    "title": "Consenso y Anotación",
    "section": "",
    "text": "El consenso de genoma es esencial para una variedad de aplicaciones en genómica y biología molecular. Algunos de los usos más comunes incluyen:\n\nEnsamblaje de Genomas: En proyectos de secuenciación de novo, donde no se dispone de una referencia genómica, el consenso de genoma se utiliza para ensamblar el genoma completo a partir de las secuencias de lecturas.\nVariación Genómica: La identificación de variantes genómicas, como mutaciones o polimorfismos, se basa en comparaciones entre el genoma de referencia y las lecturas secuenciadas.\nAnálisis de Metagenómica: En estudios de metagenómica, donde se analiza el ADN de múltiples organismos en una muestra, el consenso de genoma permite identificar y caracterizar las especies presentes.\nEstudios de Filogenia: La construcción de árboles filogenéticos y la comparación de genomas entre especies se basan en secuencias de consenso.",
    "crumbs": [
      "Genomica y Metagenomica",
      "Consenso y Anotación"
    ]
  },
  {
    "objectID": "modulo9.html#proceso-de-consenso-de-genoma",
    "href": "modulo9.html#proceso-de-consenso-de-genoma",
    "title": "Consenso y Anotación",
    "section": "",
    "text": "El proceso de consenso de genoma implica la alineación y comparación de las secuencias de lecturas contra un genoma de referencia o entre ellas mismas. A medida que se realizan estas comparaciones, se determina la secuencia de consenso, que representa la secuencia de ADN más probable en las regiones cubiertas por las lecturas.",
    "crumbs": [
      "Genomica y Metagenomica",
      "Consenso y Anotación"
    ]
  },
  {
    "objectID": "modulo9.html#herramientas-y-bibliotecas",
    "href": "modulo9.html#herramientas-y-bibliotecas",
    "title": "Consenso y Anotación",
    "section": "",
    "text": "Para realizar el consenso de genoma, se utilizan herramientas y bibliotecas bioinformáticas especializadas, como Rsamtools, Samtools, Bioconductor y otras, que permiten el procesamiento eficiente de archivos de alineación (BAM/SAM) y la generación de secuencias de consenso de alta calidad.\nEl consenso de genoma es una parte esencial de la investigación en genómica y juega un papel crucial en la comprensión de la estructura y la función de los genomas. A medida que avanza la tecnología de secuenciación, las técnicas y las herramientas de consenso de genoma continúan evolucionando para abordar desafíos cada vez más complejos en la genómica.",
    "crumbs": [
      "Genomica y Metagenomica",
      "Consenso y Anotación"
    ]
  },
  {
    "objectID": "modulo9.html#generación-de-consenso-genómico",
    "href": "modulo9.html#generación-de-consenso-genómico",
    "title": "Consenso y Anotación",
    "section": "Generación de consenso genómico",
    "text": "Generación de consenso genómico\nEn esta sección, se genera un consenso genómico utilizando la biblioteca Rsamtools. Primero, se cuenta la posición de la alineación y se muestra un resumen:\nres &lt;- pileup(bamFile)\nhead(res)\ntable(res$strand, res$nucleotide)\nLuego, se genera un gráfico de cobertura:\ncover &lt;- res[,c(\"pos\",\"count\")]\nplot(count ~ pos, cover , pch =\".\")\nSe definen parámetros para el consenso genómico:\np_param &lt;- PileupParam(distinguish_strands = FALSE, \n                       distinguish_nucleotides = TRUE,\n                       min_mapq = 10,\n                       min_nucleotide_depth = 5,\n                       min_base_quality = 10,\n                       min_minor_allele_depth = 0)\nY finalmente, se genera el consenso genómico:\nres &lt;- pileup(bamFile, pileupParam = p_param)\nex &lt;- head(res, 20000)\n\ncons2  &lt;- consensus_parallel(sup = 9000, ex = res, depth = 1, \n                                    freq_threshold = 0.6, mltcore = 7, \n                             fastafile = \"results/SRR15616379.fasta\")\n\n\n\n\n\n\nComparativa con Linux\n\n\n\nEste código calcula un consenso genómico a partir de la alineación en el archivo BAM utilizando los parámetros especificados y lo almacena en un archivo FASTA en el directorio results/.\nTambien podemos ver este proceso en Linux usando el programa ivar:\n\nsamtools mpileup -A -d 0 -Q 0 SRR15616379.bam | ivar consensus -p ref -q 10 -t 0.6 -n N -m 20",
    "crumbs": [
      "Genomica y Metagenomica",
      "Consenso y Anotación"
    ]
  },
  {
    "objectID": "modulo9.html#anotación-de-genomas-bacterianos",
    "href": "modulo9.html#anotación-de-genomas-bacterianos",
    "title": "Consenso y Anotación",
    "section": "Anotación de Genomas Bacterianos",
    "text": "Anotación de Genomas Bacterianos\nEn esta sección, aprenderás a realizar la anotación de genomas bacterianos utilizando la herramienta Prokka. Asegúrate de seguir los pasos detallados a continuación.\nPaso 1: Instalación de Prokka\nAntes de comenzar con la anotación, primero debemos instalar la herramienta Prokka. Si aún no lo has hecho, puedes encontrar instrucciones detalladas en el índice del curso.\nPaso 2: Anotación con Prokka\nUna vez que tengas Prokka instalado, puedes usar el siguiente comando para realizar la anotación de tu genoma bacteriano:\n# Anotación de genomas bacterianos\nprokka --kingdom Bacteria --outdir annotation/ --centre X --compliant ref.fa\nEste comando creará una carpeta llamada annotation/ donde podrás encontrar los diferentes formatos de anotación generados por Prokka.\nPaso 3: Edición de los Datos de Anotación\nAntes de trabajar con los datos de anotación, es importante realizar algunas ediciones. A continuación, se presentan los comandos necesarios:\n# Retirar el encabezado del archivo GFF\nhead -n 2 PROKKA_10052023.gff &gt; annotation.gff\n\n# Retirar las secuencias fasta no deseadas del archivo GFF\nfgrep \"gnl|X|DKBIDJJK_1\" PROKKA_10052023.gff | fgrep -v \"&gt;gnl\" &gt;&gt; annotation.gff\ncut -d\";\" -f1 annotation.gff &gt; annotation2.gff\n\n# Retirar el encabezado del archivo TSV\nhead -n 1 PROKKA_10052023.tsv | cut -f1,2,3,4 &gt; annotation.csv\n\n# Retirar las anotaciones del archivo TSV\nawk '{print $1\"\\t\"$2\"\\t\"$3\"\\t\"$4}' PROKKA_10052023.tsv &gt;&gt; annotation.tsv\nPaso 4: Procesamiento de Datos en R\nAhora que hemos preparado los datos, podemos cargarlos en R y realizar análisis adicionales. Asegúrate de tener las bibliotecas necesarias instaladas. Puedes ejecutar estos comandos en R:\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ape)\n\n\nAttaching package: 'ape'\n\n\nThe following object is masked from 'package:dplyr':\n\n    where\n\n\n\n# Leer los datos de anotación desde un archivo GFF\ngfff &lt;- read.gff(\"data/annotation/annotation2.gff\")\ngfff$attributes &lt;- gsub(\"ID=\", \"\", gfff$attributes, fixed = TRUE)\ngfff$attributes &lt;- gsub(\"_gene\", \"\", gfff$attributes, fixed = TRUE)\n\n\n# Leer los datos de anotación desde un archivo TSV\ndf &lt;- read.table(\"data/annotation/annotation.tsv\", sep = \"\\t\", header = TRUE)\ndf &lt;- df %&gt;% dplyr::filter(ftype %in% c(\"CDS\", \"rRNA\", \"tRNA\"))\n\n\n# Filtrar y procesar los datos\ngffCDS &lt;- gfff %&gt;% dplyr::filter(type %in% c(\"CDS\", \"rRNA\", \"tRNA\"))\ntmp &lt;- merge(x = df, y = gffCDS, by.y = \"attributes\", by.x = \"locus_tag\", all = TRUE)\n\n\n# Contar el número de etiquetas de locus únicas\nlength(unique(tmp$locus_tag))\n\n[1] 1143\n\n# Filtrar y seleccionar las primeras 20 filas de genes no hipotéticos\ngenes_annot &lt;- tmp %&gt;% dplyr::filter(gene != \"hypothetical\") %&gt;% head(20)\ncolnames(genes_annot) &lt;- c(\"locus\", \"ftype\", \"width\", \"gene\", \"seqid\", \"source\", \"type\",\n                           \"start\", \"end\", \"score\", \"strand\", \"phase\")\n\nPaso 5: Visualización en Gráfico Circular\nFinalmente, podemos visualizar los datos de anotación en un gráfico circular utilizando la biblioteca circlize. Ejecuta estos comandos en R:\n\n# Cargar bibliotecas necesarias\nlibrary(circlize)\n\n========================================\ncirclize version 0.4.15\nCRAN page: https://cran.r-project.org/package=circlize\nGithub page: https://github.com/jokergoo/circlize\nDocumentation: https://jokergoo.github.io/circlize_book/book/\n\nIf you use it in published research, please cite:\nGu, Z. circlize implements and enhances circular visualization\n  in R. Bioinformatics 2014.\n\nThis message can be suppressed by:\n  suppressPackageStartupMessages(library(circlize))\n========================================\n\n\n\nAttaching package: 'circlize'\n\n\nThe following object is masked from 'package:ape':\n\n    degree\n\nlibrary(viridisLite)\nlibrary(viridis)\nset.seed(999)\n\n\n# Crear un dataframe con los datos de anotación\ndf &lt;- data.frame(\n  name  = genes_annot$gene,\n  start = genes_annot$start,\n  end   = genes_annot$end,\n  tipo = genes_annot$ftype\n)\n\n# Inicializar el gráfico circular\ncircos.genomicInitialize(df)\n\n# Configurar una pista en el gráfico circular\ncircos.track(\n  ylim = c(0, 1),  # Rango de valores en el eje Y\n  bg.col = viridis(20),  # Colores de fondo de las regiones\n  bg.border = NA,  # Sin borde en las regiones\n  track.height = 0.05  # Altura de la pista\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\nEn resumen\n\n\n\nEn esta sección, hemos cargado las bibliotecas necesarias y creado un dataframe df con los datos de anotación. Luego, inicializamos el gráfico circular utilizando circos.genomicInitialize y configuramos una pista en el gráfico circular con circos.track.",
    "crumbs": [
      "Genomica y Metagenomica",
      "Consenso y Anotación"
    ]
  },
  {
    "objectID": "modulo6.html#cómo-citar-en-r",
    "href": "modulo6.html#cómo-citar-en-r",
    "title": "Introducción a R",
    "section": "Cómo Citar en R",
    "text": "Cómo Citar en R\nSi necesitas citar R o un paquete en particular en tu trabajo, puedes utilizar los siguientes comandos:\nCitar R:\ncitation(\"base\")\nCitar un paquete en particular:\ncitation(\"NombrePaquete\")"
  },
  {
    "objectID": "modulo6.html#trabajar-con-paquetes-y-datos",
    "href": "modulo6.html#trabajar-con-paquetes-y-datos",
    "title": "Introducción a R",
    "section": "Trabajar con Paquetes y Datos",
    "text": "Trabajar con Paquetes y Datos\nPuedes explorar una amplia gama de paquetes en el repositorio CRAN (Comprehensive R Archive Network) en CRAN. Algunos paquetes están diseñados específicamente para la bioinformática, como adegenet y ape. También puedes encontrar una lista de paquetes relacionados con la genética estadística en CRAN Task Statistical Genetics.\nPara instalar un paquete en tu máquina, utiliza el comando install.packages.\n\n\n\n\n\n\nNota\n\n\n\n\n\nUna vez que el paquete está instalado, debes cargarlo en la sesión de R utilizando la función library cada vez que necesites utilizarlo en un análisis específico.",
    "crumbs": [
      "Introduccion a R",
      "Introducción a R"
    ]
  },
  {
    "objectID": "modulo6.html#instalación-y-carga-de-ggplot2",
    "href": "modulo6.html#instalación-y-carga-de-ggplot2",
    "title": "Introducción a R",
    "section": "Instalación y Carga de ggplot2",
    "text": "Instalación y Carga de ggplot2\nAntes de comenzar, asegúrate de tener instalado el paquete ggplot2. Si aún no lo has hecho, puedes instalarlo y cargarlo de la siguiente manera:\ninstall.packages(\"ggplot2\")\nlibrary(ggplot2)\n\n\n\n\n\n\nNota\n\n\n\n\n\nPara crear un gráfico básico con ggplot2, debes especificar un conjunto de datos y mapear las variables a los estéticos (como x e y) utilizando la función ggplot(). Luego, puedes agregar capas de geometrías para representar los datos.\n\n\n\n# Crear un gráfico de dispersión simple.\nggplot(data = datos, aes(x = edad, y = puntaje)) +\n  geom_point()",
    "crumbs": [
      "Introduccion a R",
      "Introducción a R"
    ]
  },
  {
    "objectID": "modulo6.html#creación-de-un-gráfico-básico",
    "href": "modulo6.html#creación-de-un-gráfico-básico",
    "title": "Introducción a R",
    "section": "Creación de un Gráfico Básico",
    "text": "Creación de un Gráfico Básico\nPara crear un gráfico básico con ggplot2, debes especificar un conjunto de datos y mapear las variables a los estéticos (como x e y) utilizando la función ggplot(). Luego, puedes agregar capas de geometrías para representar los datos.\nEjemplo:\n# Crear un gráfico de dispersión simple.\nggplot(data = datos, aes(x = edad, y = puntaje)) +\n  geom_point()"
  },
  {
    "objectID": "modulo6.html#personalización-de-gráficos",
    "href": "modulo6.html#personalización-de-gráficos",
    "title": "Introducción a R",
    "section": "Personalización de Gráficos",
    "text": "Personalización de Gráficos\nggplot2 permite una amplia personalización de los gráficos. Puedes ajustar colores, etiquetas, escalas y más.\nEjemplo:\n# Personalizar el gráfico de dispersión.\nggplot(data = datos, aes(x = edad, y = puntaje)) +\n  geom_point(color = \"blue\", size = 3) +\n  labs(title = \"Relación entre Edad y Puntaje\",\n       x = \"Edad\",\n       y = \"Puntaje\")\nggplot2 es versátil y puede crear varios tipos de gráficos, como histogramas, barras, líneas y más. Solo debes ajustar la función geom_ según el tipo de gráfico deseado.",
    "crumbs": [
      "Introduccion a R",
      "Introducción a R"
    ]
  },
  {
    "objectID": "modulo6.html#tipos-de-gráficos",
    "href": "modulo6.html#tipos-de-gráficos",
    "title": "Introducción a R",
    "section": "Tipos de Gráficos",
    "text": "Tipos de Gráficos\nggplot2 es versátil y puede crear varios tipos de gráficos, como histogramas, barras, líneas y más. Solo debes ajustar la función geom_ según el tipo de gráfico deseado.\nEjemplos:\n\nHistograma\n# Crear un histograma de la variable \"edades\".\nggplot(data = datos, aes(x = edad)) +\n  geom_histogram()\n\n\nGráfico de Barras\n# Crear un gráfico de barras para contar la frecuencia de géneros.\nggplot(data = datos, aes(x = genero)) +\n  geom_bar()\n\n\nGráfico de Líneas\n# Crear un gráfico de líneas para seguir cambios a lo largo del tiempo.\nggplot(data = datos, aes(x = tiempo, y = valor)) +\n  geom_line()"
  },
  {
    "objectID": "modulo6.html#facetas",
    "href": "modulo6.html#facetas",
    "title": "Introducción a R",
    "section": "Facetas",
    "text": "Facetas\nLas facetas te permiten dividir un gráfico en múltiples paneles según una variable. Esto es útil para explorar datos segmentados.\nEjemplo:\n# Crear un gráfico de dispersión facetado por género.\nggplot(data = datos, aes(x = edad, y = puntaje)) +\n  geom_point() +\n  facet_wrap(~genero)",
    "crumbs": [
      "Introduccion a R",
      "Introducción a R"
    ]
  },
  {
    "objectID": "modulo6.html#guardar-gráficos",
    "href": "modulo6.html#guardar-gráficos",
    "title": "Introducción a R",
    "section": "Guardar Gráficos",
    "text": "Guardar Gráficos\nPuedes guardar tus gráficos en archivos de imagen con la función ggsave().\nEjemplo:\n# Guardar el gráfico en un archivo PNG.\nggsave(\"grafico.png\", plot = p, width = 6, height = 4, units = \"in\")"
  },
  {
    "objectID": "modulo6.html#instalación-y-carga-de-ggplot2-1",
    "href": "modulo6.html#instalación-y-carga-de-ggplot2-1",
    "title": "Introducción a R",
    "section": "Instalación y Carga de ggplot2",
    "text": "Instalación y Carga de ggplot2\nAntes de comenzar, asegúrate de tener instalado el paquete ggplot2. Si aún no lo has hecho, puedes instalarlo y cargarlo de la siguiente manera:\ninstall.packages(\"ggplot2\")\nlibrary(ggplot2)\n\n\n\n\n\n\nNota\n\n\n\n\n\nPara crear un gráfico básico con ggplot2, debes especificar un conjunto de datos y mapear las variables a los estéticos (como x e y) utilizando la función ggplot(). Luego, puedes agregar capas de geometrías para representar los datos.\n\n\n\n# Crear un gráfico de dispersión simple.\nggplot(data = datos, aes(x = edad, y = puntaje)) +\n  geom_point()"
  },
  {
    "objectID": "modulo6.html#filtrar-filas",
    "href": "modulo6.html#filtrar-filas",
    "title": "Introducción a R",
    "section": "Filtrar Filas",
    "text": "Filtrar Filas\nLa función filter() se utiliza para seleccionar filas específicas de un conjunto de datos en función de condiciones específicas.\nEjemplo:\n# Filtrar datos para seleccionar solo las filas donde la columna \"edad\" es mayor o igual a 18.\nnuevos_datos &lt;- datos %&gt;% filter(edad &gt;= 18)",
    "crumbs": [
      "Introduccion a R",
      "Introducción a R"
    ]
  },
  {
    "objectID": "modulo6.html#seleccionar-columnas",
    "href": "modulo6.html#seleccionar-columnas",
    "title": "Introducción a R",
    "section": "Seleccionar Columnas",
    "text": "Seleccionar Columnas\nCon select(), puedes elegir las columnas que deseas incluir en tu conjunto de datos y descartar las que no necesitas.\nEjemplo:\n# Seleccionar solo las columnas \"nombre\" y \"edad\" del conjunto de datos.\ndatos_seleccionados &lt;- datos %&gt;% select(nombre, edad)",
    "crumbs": [
      "Introduccion a R",
      "Introducción a R"
    ]
  },
  {
    "objectID": "modulo6.html#crear-nuevas-columnas",
    "href": "modulo6.html#crear-nuevas-columnas",
    "title": "Introducción a R",
    "section": "Crear Nuevas Columnas",
    "text": "Crear Nuevas Columnas\nLa función mutate() te permite crear nuevas columnas en tus datos basadas en cálculos o transformaciones de las columnas existentes.\nEjemplo:\n# Crear una nueva columna \"edad_doble\" que contenga el doble de la edad.\ndatos_con_nueva_columna &lt;- datos %&gt;% mutate(edad_doble = edad * 2)",
    "crumbs": [
      "Introduccion a R",
      "Introducción a R"
    ]
  },
  {
    "objectID": "modulo6.html#agregar-y-resumir-datos",
    "href": "modulo6.html#agregar-y-resumir-datos",
    "title": "Introducción a R",
    "section": "Agregar y Resumir Datos",
    "text": "Agregar y Resumir Datos\nPara realizar agregaciones y resúmenes de datos, group_by() se utiliza para agrupar datos en función de una o más columnas, y summarize() permite calcular estadísticas resumidas para cada grupo.\nEjemplo:\n# Calcular la media de la edad para cada género.\nresumen_edad_por_genero &lt;- datos %&gt;% \n  group_by(genero) %&gt;% \n  summarize(media_edad = mean(edad))",
    "crumbs": [
      "Introduccion a R",
      "Introducción a R"
    ]
  },
  {
    "objectID": "modulo6.html#ordenar-filas",
    "href": "modulo6.html#ordenar-filas",
    "title": "Introducción a R",
    "section": "Ordenar Filas",
    "text": "Ordenar Filas\nLa función arrange() se utiliza para ordenar las filas de un conjunto de datos en función de una o más columnas.\nEjemplo:\n# Ordenar el conjunto de datos por la columna \"edad\" de forma ascendente.\ndatos_ordenados &lt;- datos %&gt;% arrange(edad)",
    "crumbs": [
      "Introduccion a R",
      "Introducción a R"
    ]
  },
  {
    "objectID": "modulo6.html#filtrar-filas-únicas",
    "href": "modulo6.html#filtrar-filas-únicas",
    "title": "Introducción a R",
    "section": "Filtrar Filas Únicas",
    "text": "Filtrar Filas Únicas\nSi deseas eliminar filas duplicadas y quedarte solo con filas únicas, distinct() es la función adecuada.\nEjemplo:\n# Eliminar filas duplicadas basadas en la columna \"nombre\".\ndatos_unicos &lt;- datos %&gt;% distinct(nombre)",
    "crumbs": [
      "Introduccion a R",
      "Introducción a R"
    ]
  },
  {
    "objectID": "modulo6.html#histograma",
    "href": "modulo6.html#histograma",
    "title": "Introducción a R",
    "section": "Histograma",
    "text": "Histograma\n# Crear un histograma de la variable \"edades\".\nggplot(data = datos, aes(x = edad)) +\n  geom_histogram()",
    "crumbs": [
      "Introduccion a R",
      "Introducción a R"
    ]
  },
  {
    "objectID": "modulo6.html#gráfico-de-barras",
    "href": "modulo6.html#gráfico-de-barras",
    "title": "Introducción a R",
    "section": "Gráfico de Barras",
    "text": "Gráfico de Barras\n# Crear un gráfico de barras para contar la frecuencia de géneros.\nggplot(data = datos, aes(x = genero)) +\n  geom_bar()",
    "crumbs": [
      "Introduccion a R",
      "Introducción a R"
    ]
  },
  {
    "objectID": "modulo6.html#gráfico-de-líneas",
    "href": "modulo6.html#gráfico-de-líneas",
    "title": "Introducción a R",
    "section": "Gráfico de Líneas",
    "text": "Gráfico de Líneas\n# Crear un gráfico de líneas para seguir cambios a lo largo del tiempo.\nggplot(data = datos, aes(x = tiempo, y = valor)) +\n  geom_line()",
    "crumbs": [
      "Introduccion a R",
      "Introducción a R"
    ]
  },
  {
    "objectID": "modulo7.html#análisis-de-datos-ngs-illumina",
    "href": "modulo7.html#análisis-de-datos-ngs-illumina",
    "title": "Fundamentos de NGS",
    "section": "Análisis de datos NGS illumina",
    "text": "Análisis de datos NGS illumina\n\nControl de Calidad\nAntes de saltar a filtrar tus datos con filtros de calidad que la terminal ejecute muy obediente, lo mejor es ver algunos gráficos básicos que nos dicen mucho más que una serie de caracteres ASCII. Usaremos para ello FASTQC que es un programa que hace una serie de análisis básicos y estándar decalidad.\n\n\n\n\n\n\nParámetros en FastQC\n\n\n\n\n\nLos análisis de FASTQC son útiles para identificar problemas que pudieron surgir durante el laboratorio o durante la secuenciación.\nEl análisis de FASTQC consiste en los siguientes campos:\n\nBasic Statistics\nPer Base Sequence Quality\nPer Sequence Quality Scores\nPer Base Sequence Content\nPer Sequence GC Content\nPer Base N Content\nSequence Length Distribution\nDuplicate Sequences\nOverrepresented Sequences\nAdapter Content\nKmer Content\nPer Tile Sequence Quality\n\n\n\n\n\n\n\n\n\n\nNotas importantes\n\n\n\n\n\n\nFASTQ automáticamente dice si nuestra muestra “pasó” o “falló” la evaluación. Sin embargo debemos tomar esto dentro del contexto de lo que esperamos de nuestra librería, ya que FASTQ espera una distribución diversa y al azar de nucleótidos, lo que puede no cumplirse en algunos protocolos.\n\n\n\n\nVamos a la página de dicho programa a ver ejemplos de:\n\nBuenos datos Illumina\nMalos datos Illumina\nCorrida Illumina contaminada con dímeros de adaptadores (adapter dimers)\n\n\n\n\n\n\n\nDímeros de adaptadores\n\n\n\n\n\n¿Qué que son los dímeros de adaptadores?\nLos adaptadores se ligan al ADN de nuestras muestras en un paso de ligación, sin embargo, también pueden ligarse entre sí y luego pegarse a la flow cell. Resultado: son secuenciados pero no proven datos útiles, simplemente la secuencia de los adaptadores repetida muchas veces. Adelante veremos cómo lidiar con ellos bioinformáticamente, pero se recomienda intentar deshacerse de ellos desde el laboratorio (con pequeños, pequeños imanes como Agencourt o símiles de otras marcas).\n\n\n\n\n\n\n\n\n\nImportancia de Fastqc\n\n\n\n\n\n¿Qué tanto importa el análisis FASTQC?\nMucho, a partir del análisis FASTQC es que decidirás si tu secuenciación fue exitosa y qué parámetros de pre-procesamiento deberás utilizar para deshacerte del ruido y quedarte con datos limpios.\nEscoger los parámetros adecuados de pre-procesamiento es vital ya que todas las corridas de secuenciación son diferentes. Además entender bien tu FASTQC puede permitirte rescatar datos usables incluso dentro de una mala corrida.\n\n\n\n\n\nComando de Fastqc en Linux\nRecuerda moverte a la ubicacion donde se encuentras los archivos Fastq con el comando cd y la direccion.\nmkdir -p results/quality\nfastqc -t 2 *fastq.gz -o results/quality/\n\n\n\n\n\n\nAclaración\n\n\n\n\nDurante el curso veremos que estos comandos pueden aplicarse en R y complementaremos con comandos en Linux.\n\n\n\n\n\nFiltrado de Reads\n\nPre-procesamiento\nEl pre-procesamiento es esencial para obtener datos limpios a partir de datos crudos en el campo de la bioinformática. Se utiliza principalmente en el análisis de datos biológicos y se inicia con archivos .fastq como entrada, generando también archivos .fastq como salida, posiblemente en versiones comprimidas.\n\n\n\n\n\n\nPasos comunes pre-procesamiento\n\n\n\n\n\n\nRecortar secuencias por calidad (Sequence Quality Trimming): Elimina las bases de baja calidad, típicamente en las últimas bases (-3’ end) de secuencias. La cantidad de bases a recortar se decide visualmente o con un parámetro de calidad, utilizando herramientas como FASTQC.\nRecortar secuencias (Trimming): Elimina una cantidad específica de bases que no son relevantes para el análisis, como códigos de barras o sitios de restricción.\nFiltrar secuencias por calidad: Descarta las secuencias que no cumplen con ciertos estándares de calidad, como un número mínimo de bases con calidad o un promedio de calidad específico.\nQuitar adaptadores: Busca secuencias de adaptadores y las elimina de las secuencias finales. También es posible limitar las secuencias finales a aquellas con un adaptador específico.\nFiltrar artefactos: Identifica y elimina primers de PCRs, quimeras y otros artefactos de los datos finales.\nSeparar por barcodes “demultiplexear”: Identifica y separa secuencias que contienen códigos de barras (barcodes) únicos por muestra, permitiendo la distinción de secuencias de diferentes muestras. Esto requiere una lista de barcodes y su ubicación en las secuencias.\nPaired end merging: En el caso de secuenciación Illumina en ambos lados (pair end), se unen las lecturas si se detecta que coinciden, lo que permite corregir errores de secuenciación utilizando la base de mayor calidad.\nRemover otras secuencias no deseadas: Busca y elimina secuencias no deseadas, como genoma de E. coli, restos de PhiX o partes del genoma que no son de interés, como cpDNA.\n\n\n\n\n\n\n\n\n\n\nAdvertencia\n\n\n\n\n\nEs importante destacar que la lista de barcodes y nombres de muestras es información crítica en un proyecto de bioinformática y debe manejarse con cuidado.\n\n\n\n\n\n\nComando de Trim_galore en Linux\n#trim_galore \ntrim_galore --quality 20 --fastqc --length 250 --output_dir *.gz\n\nDescargar secuencia de ejemplo Fastq\n\nRBASH\n\n\ndownload.file(\"ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR156/080/SRR15616380/SRR15616380_1.fastq.gz\", \"SRR15616380_1.fastq.gz\")\n\ndownload.file(\"ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR156/080/SRR15616380/SRR15616380_2.fastq.gz\", \"SRR15616380_2.fastq.gz\")\n\n\nwget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR156/080/SRR15616380/SRR15616380_1.fastq.gz\nwget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR156/080/SRR15616380/SRR15616380_2.fastq.gz",
    "crumbs": [
      "Fundamentos NGS",
      "Fundamentos de NGS"
    ]
  },
  {
    "objectID": "modulo8.html#funciones-y-archivos-fuente-utilizados",
    "href": "modulo8.html#funciones-y-archivos-fuente-utilizados",
    "title": "Genómica con R",
    "section": "Funciones y archivos Fuente Utilizados",
    "text": "Funciones y archivos Fuente Utilizados\nEn esta sección, se colocaran las funciones y los archivos fuentes que utilizaremos durante el curso.\n\nDescarga de SRA Toolkit\nEl siguiente código descarga SRA Toolkit dependiendo del sistema operativo (Windows o Linux) y configura las funciones necesarias para su uso.\n\n\n\n\n\n\nscripts/sratoolkit.R\n\n\n\n\n\n# Código para descargar SRA Toolkit y configurar funciones específicas\n# según el sistema operativo (Windows o Linux)\n# ...\n\n# Funciones para descargar y dividir archivos SRA\nsetClass(\"sratoolkit\", slots = list(dest = \"character\", sraid = \"character\"))\n\nsetGeneric(\"downloadsra\", function(obj) {\n  standardGeneric(\"downloadsra\")\n})\n\nsetGeneric(\"fastqdump\", function(obj, output) {\n  standardGeneric(\"fastqdump\")\n})\n\n\n\n\n\nListado de archivos FASTQ\nEl archivo listfastq.R contiene una función llamada list_fastq que se utiliza para listar los archivos FASTQ en un directorio específico y organizarlos según un patrón.\n\n\n\n\n\n\nscripts/listfastq.R\n\n\n\n\n\nlist_fastq &lt;- function(ruta = getwd(), pattern = c(\"_1.fastq\", \"_2.fastq\")){\n  #Set working directory\n  setwd(ruta)\n  fastq= paste0(ruta,\"/data/rawdata\")\n  list.files(fastq)\n  lecturasf &lt;- sort(list.files(fastq, pattern = pattern[1], full.names = T))\n  lecturasr &lt;- sort(list.files(fastq, pattern = pattern[2], full.names = T))\n  nombres_muestras &lt;- sapply(strsplit(basename(lecturasf),\"_\"), `[`,1)\n  return(list(lf = lecturasf, lr = lecturasr, name = nombres_muestras))\n  \n}\n\n\n\n\n\nFiltrado y recorte de lecturas\nEl archivo filterreads.R contiene una función llamada filter_reads que se utiliza para filtrar y recortar lecturas de archivos FASTQ.\n\n\n\n\n\n\nscripts/filtereads.R\n\n\n\n\n\n\n#----filtrado y recorte de lecturas----\n\nfilter_reads &lt;- function(name, lf, lr, trunc = 150){\n  \n  filtF &lt;- file.path(paste0(getwd(),\"/data/processed_data\"), \"filtered_F\", \n                     paste0(name, \"_filt_1.fastq\"))\n  filtR &lt;- file.path(paste0(getwd(),\"/data/processed_data\"), \"filtered_R\", \n                     paste0(name, \"_filt_2.fastq\"))\n  names(filtF) &lt;- name\n  names(filtR) &lt;- name\n\n  out &lt;- filterAndTrim(lf,filtF,lr,filtR, \n                       truncLen = c(trunc,trunc), \n                       maxN = 0,\n                       maxEE = c(2,2),\n                       truncQ = 2,\n                       rm.phix = T,\n                       compress = F,\n                       multithread = T)\n  \n  recovery &lt;- round(out[2]/out[1]*100,2)\n  log &lt;- paste0(\"Secuencias recuperadas : \",recovery, \"%\")\n  cat(log)\n  return(log)\n}\n\n\n\n\n\nGeneración de consenso\nEl archivo consensus.R contiene una función llamada consensus_parallel que se utiliza para generar secuencias de consenso a partir de datos de secuenciación. La función consensus_parallel toma varios argumentos, incluyendo el rango de posiciones, profundidad, umbral de frecuencia y otros.\n\n\n\n\n\n\nscripts/filtereads.R\n\n\n\n\n\n\nconsensus_parallel &lt;- function(sup, inf=1, ex, depth=0, freq_threshold = 0.6, \n                               mltcore, fastafile = NULL){\n  \n  nucl_string &lt;-function(num, ex,depth,freq_threshold ){\n    ## exist position \n    if(nrow(ex[which(ex$pos == num), ]) == 0){\n      return &lt;- \"N\"\n    }else{\n      tmp &lt;- ex[which(ex$pos ==num), ]\n      ## depth &gt;= threshold\n      if(sum(tmp$count) &gt;= depth){\n        tmp &lt;- tmp %&gt;% dplyr::mutate(Percentage = count/sum(count))\n        ### most frequency nucl &gt;= threshold \n        if(max(tmp$Percentage) &gt;= freq_threshold){\n          tmp &lt;- as.character(tmp[which(tmp$Percentage == max(tmp$Percentage)),3])\n          return &lt;- tmp\n          ### most frequency nucl &lt; threshold \n        }else{\n          \n          ## capture two more frequency nucl \n          abg &lt;- as.character(tmp[which(tmp$Percentage &gt;= freq_threshold/2),3])\n          abg_rev &lt;- rev(abg)\n          ## make a string normal and rev\n          abg &lt;-  paste(abg, collapse = \"\")\n          abg_rev &lt;- paste(abg_rev, collapse = \"\")\n          \n          ## identify IUPAC code \n          result = switch(  \n            abg,  \n            \"AG\"= \"R\",  \n            \"CT\"= \"Y\",  \n            \"GC\"= \"S\",  \n            \"AT\"= \"W\",  \n            \"GT\"= \"K\",  \n            \"AC\"= \"M\"\n          )  \n          \n          result_rev = switch(  \n            abg_rev,  \n            \"AG\"= \"R\",  \n            \"CT\"= \"Y\",  \n            \"GC\"= \"S\",  \n            \"AT\"= \"W\",  \n            \"GT\"= \"K\",  \n            \"AC\"= \"M\"\n          )  \n          \n          ## assign code to vector consensus\n          if(is.null(result_rev) && is.null(result)){\n            return &lt;-  \"N\"\n          }else{\n            r &lt;- c(result,result_rev)\n            return &lt;-  r\n          }\n          \n        }\n        \n        ## depth &lt;= threshold   \n      }else{\n        return &lt;-  \"N\"\n      }\n    }\n  }\n  \n  # dna_vector &lt;- c()\n  \n  my.cluster &lt;- parallel::makeCluster(\n    mltcore, \n    type = \"PSOCK\"\n  )\n  \n  doParallel::registerDoParallel(cl = my.cluster)\n  foreach::getDoParRegistered()\n  foreach::getDoParWorkers()\n  \n  x &lt;- foreach(i = 1:sup, .combine = 'c',.packages='dplyr')%dopar%{\n    nucl_string(num = i, ex = ex,depth = depth,\n                freq_threshold = freq_threshold )\n  }\n  \n  #make string of vector consensus\n  dna_string &lt;- paste(x, collapse = \"\")\n  parallel::stopCluster(cl = my.cluster)\n  \n  if(!is.null(fastafile)){\n    fileConn&lt;-file(fastafile)\n    writeLines(c(paste0(\"&gt;\",as.character(res$seqnames[1]),\n                        \"_\",format(inf,scientific=FALSE),\n                        \":\",format(sup,scientific=FALSE)),\n                 dna_string), fileConn)\n    close(fileConn)\n  }\n  return(x)\n}",
    "crumbs": [
      "Fundamentos NGS",
      "Genómica con R"
    ]
  },
  {
    "objectID": "modulo8.1.html",
    "href": "modulo8.1.html",
    "title": "Pipeline de Genómica en R",
    "section": "",
    "text": "En esta sección, se realiza la descarga de un genoma desde la base de datos del NCBI utilizando el código proporcionado:\nurl &lt;- fromJSON(file = \"data/reference/ref.json\")\nfilename &lt;- \"GCF_000085865.1_ASM8586v1_genomic.fna.gz\"\ndownload_genome(url$urlref, filename, \"data/reference/\")\n\n\n\n\n\n\nAclaración\n\n\n\nEl código utiliza el archivo JSON ref.json para obtener la URL del genoma y luego descarga el genoma en formato FASTA comprimido. Los archivos de genoma descargados se almacenan en el directorio data/reference/.",
    "crumbs": [
      "Fundamentos NGS",
      "Pipeline de Genómica en R"
    ]
  },
  {
    "objectID": "modulo8.1.html#descarga-de-genomas-desde-ncbi",
    "href": "modulo8.1.html#descarga-de-genomas-desde-ncbi",
    "title": "Pipeline de Genómica en R",
    "section": "",
    "text": "En esta sección, se realiza la descarga de un genoma desde la base de datos del NCBI utilizando el código proporcionado:\nurl &lt;- fromJSON(file = \"data/reference/ref.json\")\nfilename &lt;- \"GCF_000085865.1_ASM8586v1_genomic.fna.gz\"\ndownload_genome(url$urlref, filename, \"data/reference/\")\n\n\n\n\n\n\nAclaración\n\n\n\nEl código utiliza el archivo JSON ref.json para obtener la URL del genoma y luego descarga el genoma en formato FASTA comprimido. Los archivos de genoma descargados se almacenan en el directorio data/reference/.",
    "crumbs": [
      "Fundamentos NGS",
      "Pipeline de Genómica en R"
    ]
  },
  {
    "objectID": "modulo8.1.html#descarga-de-archivos-desde-sra---ncbi",
    "href": "modulo8.1.html#descarga-de-archivos-desde-sra---ncbi",
    "title": "Pipeline de Genómica en R",
    "section": "Descarga de archivos desde SRA - NCBI",
    "text": "Descarga de archivos desde SRA - NCBI\nEsta sección demuestra cómo descargar archivos desde la Base de Datos de Archivos de Secuencia (SRA) de NCBI utilizando la función download_sra_files:\ndownload_sra_files(id = \"SRR15616379\")\ndownload_sra_files(id = \"SRR26204001\")\n\n\n\n\n\n\nEn resumen\n\n\n\nEl código descarga archivos SRA utilizando los identificadores específicos proporcionados (SRR15616379 y SRR26204001) y luego los convierte en archivos FASTQ en el directorio especificado. Ver Aqui"
  },
  {
    "objectID": "modulo8.1.html#control-de-calidad-de-archivos-fastq",
    "href": "modulo8.1.html#control-de-calidad-de-archivos-fastq",
    "title": "Pipeline de Genómica en R",
    "section": "Control de calidad de archivos FASTQ",
    "text": "Control de calidad de archivos FASTQ\nEn esta sección, se realiza un control de calidad de los archivos FASTQ descargados utilizando la biblioteca fastqc. Primero, se listan las lecturas FASTQ y se genera un perfil de calidad:\nlecturas &lt;- list_fastq(pattern = c(\"SRR15616379_1.fastq.gz\", \"SRR15616379_2.fastq.gz\"))\nLuego, se ejecuta fastqc para realizar un control de calidad más detallado:\n\nlibrary(fastqcr)\nfastqc(fq.dir = \"data/rawdata/\", # Directorio de archivos FASTQ\n       qc.dir = \"results/\", # Directorio de resultados\n       threads = 4,\n       fastqc.path = \"/home/fascue/Descargas/FastQC/fastqc\"\n)\n\nEste código genera informes de calidad en el directorio results/.",
    "crumbs": [
      "Fundamentos NGS",
      "Pipeline de Genómica en R"
    ]
  },
  {
    "objectID": "modulo8.1.html#informe-de-calidad-multiqc",
    "href": "modulo8.1.html#informe-de-calidad-multiqc",
    "title": "Pipeline de Genómica en R",
    "section": "Informe de calidad multiqc",
    "text": "Informe de calidad multiqc\nPara generar un informe de calidad consolidado utilizando MultiQC, se utiliza el siguiente código:\nqc_report(qc.dir, result.file = \"results/multiqc/\",\n          experiment = \"Mycoplasma\")\n\n\n\n\n\n\nExpliación\n\n\n\n\n\nEste código genera un informe de calidad consolidado en el directorio results/multiqc/ utilizando la información de calidad previamente generada por FastQC.\nAdemás, se muestran ejemplos de cómo realizar varios tipos de análisis y visualizaciones de calidad utilizando la biblioteca qc_report. Estos ejemplos incluyen el análisis de contenido de GC, calidad de secuencia base por base, duplicación de secuencias y más.\n\n\n\nPlots de calidad FastQC\n\n\n\n\n\n\nPer base sequence quality\n\n\n\n\n\n\nqc &lt;- qc_read(\"results/SRR15616379_1_fastqc.zip\")\n\nReading: results/SRR15616379_1_fastqc.zip\n\n\nRows: 10 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (3): status, module, sample\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 8 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (2): Measure, Value\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 58 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (1): Base\ndbl (6): Mean, Median, Lower Quartile, Upper Quartile, 10th Percentile, 90th...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 13 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\ndbl (2): Quality, Count\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 58 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (1): Base\ndbl (4): G, A, T, C\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 101 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\ndbl (2): GC Content, Count\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 58 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (1): Base\ndbl (1): N-Count\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 41 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (1): Length\ndbl (1): Count\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 16 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (1): Duplication Level\ndbl (1): Percentage of total\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 0 Columns: 0\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 56 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (1): Position\ndbl (6): Illumina Universal Adapter, Illumina Small RNA 3' Adapter, Illumina...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 0 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (2): Total Deduplicated Percentage, 85.14753304887951\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nqc_plot(qc, \"Per base sequence quality\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nPer sequence quality scores\n\n\n\n\n\n\nqc_plot(qc, \"Per sequence quality scores\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nPer base sequence content\n\n\n\n\n\n\nqc_plot(qc, \"Per base sequence content\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nPer sequence GC content\n\n\n\n\n\n\nqc_plot(qc, \"Per sequence GC content\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nSequence duplication levels\n\n\n\n\n\n\nqc_plot(qc, \"Sequence duplication levels\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nOverrepresented sequences\n\n\n\n\n\n\nqc_plot(qc, \"Overrepresented sequences\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdapter content\n\n\n\n\n\n\nqc_plot(qc, \"Adapter content\")"
  },
  {
    "objectID": "modulo8.1.html#filtrado-de-lecturas-de-archivos-fastq",
    "href": "modulo8.1.html#filtrado-de-lecturas-de-archivos-fastq",
    "title": "Pipeline de Genómica en R",
    "section": "Filtrado de lecturas de archivos FASTQ",
    "text": "Filtrado de lecturas de archivos FASTQ\nEn esta sección, se realiza el filtrado de lecturas de los archivos FASTQ utilizando la función filter_reads previamente definida:\nlog_filter &lt;- filter_reads(name = lecturas$name, lf = lecturas$lf, \n             lr = lecturas$lr, trunc = 250)\nEl código ejecuta la función filter_reads en las lecturas previamente listadas, aplicando un valor de truncamiento de 250. El registro del filtrado se almacena en la variable log_filter.",
    "crumbs": [
      "Fundamentos NGS",
      "Pipeline de Genómica en R"
    ]
  },
  {
    "objectID": "modulo8.1.html#ensamblaje-de-genomas",
    "href": "modulo8.1.html#ensamblaje-de-genomas",
    "title": "Pipeline de Genómica en R",
    "section": "Ensamblaje de genomas",
    "text": "Ensamblaje de genomas\nEn esta sección, se realiza el ensamblaje de genomas utilizando la biblioteca bowtie2. Primero, se descomprime el genoma de referencia:\ngunzip(\"data/reference/GCF_000085865.1_ASM8586v1_genomic.fna.gz\")\nLuego, se construye el índice de referencia para bowtie2:\nbowtie2_build(\"data/reference/GCF_000085865.1_ASM8586v1_genomic.fna\",\n              bt2Index = \"data/reference/index/myco\" , overwrite = TRUE)\nFinalmente, se realiza el alineamiento de los archivos FASTQ a la referencia utilizando bowtie2:\nbowtie2(bt2Index = \"data/reference/index/myco\", \n        samOutput = \"results/SRR15616379.sam\", \n        seq1 = \"data/processed_data/filtered_F/SRR15616379_filt_1.fastq\", \n        seq2 = \"data/processed_data/filtered_R/SRR15616379_filt_2.fastq\", \n        \"--threads=3\")\n\n\n\n\n\n\nExplicación de proceso\n\n\n\nEstos pasos incluyen la construcción del índice de referencia y el alineamiento de las lecturas de secuenciación en los archivos FASTQ a la referencia genómica. El resultado se almacena en un archivo SAM en el directorio results/.",
    "crumbs": [
      "Fundamentos NGS",
      "Pipeline de Genómica en R"
    ]
  },
  {
    "objectID": "modulo8.1.html#manipulación-de-archivos-de-alineación",
    "href": "modulo8.1.html#manipulación-de-archivos-de-alineación",
    "title": "Pipeline de Genómica en R",
    "section": "Manipulación de archivos de alineación",
    "text": "Manipulación de archivos de alineación\nEn esta sección, se realizan diversas operaciones en archivos de alineación BAM utilizando la biblioteca Rsamtools.\nConversión a archivos BAM\nPrimero, se convierte el archivo SAM previamente generado en un archivo BAM:\nasBam(\"results/SRR15616379.sam\")\nLectura de archivo BAM\nSe carga el archivo BAM para realizar estadísticas de alineación:\n\nlibrary(Rsamtools)\n\nLoading required package: GenomeInfoDb\n\n\nLoading required package: BiocGenerics\n\n\n\nAttaching package: 'BiocGenerics'\n\n\nThe following objects are masked from 'package:stats':\n\n    IQR, mad, sd, var, xtabs\n\n\nThe following objects are masked from 'package:base':\n\n    anyDuplicated, aperm, append, as.data.frame, basename, cbind,\n    colnames, dirname, do.call, duplicated, eval, evalq, Filter, Find,\n    get, grep, grepl, intersect, is.unsorted, lapply, Map, mapply,\n    match, mget, order, paste, pmax, pmax.int, pmin, pmin.int,\n    Position, rank, rbind, Reduce, rownames, sapply, setdiff, sort,\n    table, tapply, union, unique, unsplit, which.max, which.min\n\n\nLoading required package: S4Vectors\n\n\nLoading required package: stats4\n\n\n\nAttaching package: 'S4Vectors'\n\n\nThe following object is masked from 'package:utils':\n\n    findMatches\n\n\nThe following objects are masked from 'package:base':\n\n    expand.grid, I, unname\n\n\nLoading required package: IRanges\n\n\nLoading required package: GenomicRanges\n\n\nLoading required package: Biostrings\n\n\nLoading required package: XVector\n\n\n\nAttaching package: 'Biostrings'\n\n\nThe following object is masked from 'package:base':\n\n    strsplit\n\nbamFile &lt;- BamFile(\"results/SRR15616379.bam\")\n\nEstadísticas de alineación\nSe calculan estadísticas de alineación a partir del archivo BAM:\n\nbam &lt;- countBam(bamFile)\nquickBamFlagSummary(bamFile)\n\n                                group |    nb of |    nb of | mean / max\n                                   of |  records |   unique | records per\n                              records | in group |   QNAMEs | unique QNAME\nAll records........................ A |   377410 |   188705 |    2 / 2\n  o template has single segment.... S |        0 |        0 |   NA / NA\n  o template has multiple segments. M |   377410 |   188705 |    2 / 2\n      - first segment.............. F |   188705 |   188705 |    1 / 1\n      - last segment............... L |   188705 |   188705 |    1 / 1\n      - other segment.............. O |        0 |        0 |   NA / NA\n\nNote that (S, M) is a partitioning of A, and (F, L, O) is a partitioning of M.\nIndentation reflects this.\n\nDetails for group M:\n  o record is mapped.............. M1 |   322599 |   167765 | 1.92 / 2\n      - primary alignment......... M2 |   322599 |   167765 | 1.92 / 2\n      - secondary alignment....... M3 |        0 |        0 |   NA / NA\n  o record is unmapped............ M4 |    54811 |    33871 | 1.62 / 2\n\nDetails for group F:\n  o record is mapped.............. F1 |   166780 |   166780 |    1 / 1\n      - primary alignment......... F2 |   166780 |   166780 |    1 / 1\n      - secondary alignment....... F3 |        0 |        0 |   NA / NA\n  o record is unmapped............ F4 |    21925 |    21925 |    1 / 1\n\nDetails for group L:\n  o record is mapped.............. L1 |   155819 |   155819 |    1 / 1\n      - primary alignment......... L2 |   155819 |   155819 |    1 / 1\n      - secondary alignment....... L3 |        0 |        0 |   NA / NA\n  o record is unmapped............ L4 |    32886 |    32886 |    1 / 1\n\nseqinfo(bamFile)\n\nSeqinfo object with 1 sequence from an unspecified genome:\n  seqnames    seqlengths isCircular genome\n  NC_013511.1     665445         NA   &lt;NA&gt;\n\n\nConteo de profundidad por posición\n\n# count position of alignment \nres &lt;- pileup(bamFile)\n\nhead(res)\n\n     seqnames pos strand nucleotide count\n1 NC_013511.1   1      +          A     1\n2 NC_013511.1   1      +          C     1\n3 NC_013511.1   1      -          C     1\n4 NC_013511.1   1      +          G     1\n5 NC_013511.1   1      +          T     6\n6 NC_013511.1   1      -          T     9\n\ntable(res$strand, res$nucleotide)\n\n   \n         A      C      G      T      N      =      -      +\n  + 259168  97674 131968 236724      0      0   4042      0\n  - 239533 127380  97801 256096      0      0   3281      0\n  *      0      0      0      0      0      0      0      0\n\n# coverage plot\n\ncover &lt;- res[,c(\"pos\",\"count\")]\n\n#plot(count ~ pos, cover , pch =\".\")\n\nPlot de profundidad\n\nlibrary(ggplot2)\n# Crear un gráfico de cobertura utilizando ggplot2\nggplot(cover, aes(x = pos, y = count)) +\n  geom_point(shape = \".\", size = 1) +\n  labs(title = \"Gráfico de Cobertura\",\n       x = \"Posición\",\n       y = \"Cobertura\") +\n  theme_minimal()\n\n\n\n\n\n\n\nManipulación de archivos BAM de gran tamaño\nSe muestra cómo manejar archivos BAM de gran tamaño ajustando el tamaño de lectura:\nyieldSize(bamFile) &lt;- 1\nopen(bamFile)\nscanBam(bamFile)[[1]]$seq\nclose(bamFile)\nyieldSize(bamFile) &lt;- NA",
    "crumbs": [
      "Fundamentos NGS",
      "Pipeline de Genómica en R"
    ]
  },
  {
    "objectID": "metagenomica.html",
    "href": "metagenomica.html",
    "title": "Introducción a Metagenomica",
    "section": "",
    "text": "La metagenómica es una disciplina que se enfoca en el estudio del material genético contenido en muestras ambientales o comunidades microbianas complejas. En lugar de secuenciar un solo genoma, la metagenómica permite analizar todos los genes presentes en una muestra, lo que facilita la caracterización de la diversidad y función de la microbiota de un ambiente determinado."
  },
  {
    "objectID": "metagenomica.html#cargar-librerias-y-scripts",
    "href": "metagenomica.html#cargar-librerias-y-scripts",
    "title": "Metagenomica",
    "section": "Cargar librerias y scripts",
    "text": "Cargar librerias y scripts"
  },
  {
    "objectID": "metagenomica.html#descargar-secuencias",
    "href": "metagenomica.html#descargar-secuencias",
    "title": "Metagenomica",
    "section": "Descargar secuencias",
    "text": "Descargar secuencias\ndownload_sra_files(id = \"SRR22712782\")\ndownload_sra_files(id = \"SRR22712783\")\ndownload_sra_files(id = \"SRR22712784\")\ndownload_sra_files(id = \"SRR22712785\")\ndownload_sra_files(id = \"SRR22712786\")"
  },
  {
    "objectID": "metagenomica.html#calidad-y-filtrado",
    "href": "metagenomica.html#calidad-y-filtrado",
    "title": "Metagenomica",
    "section": "calidad y filtrado",
    "text": "calidad y filtrado\nlecturas &lt;- list_fastq(pattern = c(\"_1.fastq\",\"_2.fastq\"))\nplotQualityProfile(c(lecturas$lf[3],lecturas$lr[3]))\n\n############### Filter reads from fastq files ###############\n#############################################################\n\nlog_filter &lt;- filter_reads(name = lecturas$name, lf = lecturas$lf, \n                           lr = lecturas$lr, trunc = 300)"
  },
  {
    "objectID": "metagenomica.html#modelar-los-errores-de-secuenciación",
    "href": "metagenomica.html#modelar-los-errores-de-secuenciación",
    "title": "Introducción a Metagenomica",
    "section": "Modelar los errores de secuenciación",
    "text": "Modelar los errores de secuenciación\n\nlearnErrors() es un paso crítico en el procesamiento de datos de secuenciación de amplicones, ya que permite modelar y comprender los errores de secuenciación en las secuencias\n\nfiltF &lt;- file.path(\"data/processed_data/filtered_F\", paste0(lecturas$name, \"_filt_1.fastq\"))\nfiltR &lt;- file.path(\"data/processed_data/filtered_R\", paste0(lecturas$name, \"_filt_2.fastq\"))\n\nerrR &lt;-learnErrors(filtR, multithread = T)\nerrF &lt;-learnErrors(filtF, multithread = T)\n  \n  #grafica del modelo de error\nplotErrors(errF,nominalQ = T)\nplotErrors(errR,nominalQ = T)"
  },
  {
    "objectID": "metagenomica.html#denoising-data",
    "href": "metagenomica.html#denoising-data",
    "title": "Introducción a Metagenomica",
    "section": "Denoising data",
    "text": "Denoising data\n## merge o pariar o fusion de secuencias----\npareadas &lt;- mergePairs(dadaF, filtF, dadaR, filtR, verbose = T)\n\n## construir tabla de secuencias----\nseqTab &lt;- makeSequenceTable(pareadas)\n\n## eliminar quimeras\nseqtab_nochim &lt;- removeBimeraDenovo(seqTab, method = \"consensus\"\n                                    , multithread = T, \n                                    verbose = T)\nrownames(seqtab_nochim) &lt;- sub(\"_filt_1.fastq\", \"\", rownames(seqtab_nochim), fixed = TRUE)\n\nwrite.csv2(seqtab_nochim,paste0(getwd(),\"/results/seqtab_nochim.csv\"))\n\n\n\n\n\n\nResumen\n\n\n\n\n\nEn resumen, la función dada() es un paso crítico en el procesamiento de datos de metagenómica, ya que se encarga de eliminar errores y generar ASVs a partir de secuencias de amplicones. Esto mejora significativamente la calidad de los datos y la precisión en la representación de la diversidad microbiana en una muestra."
  },
  {
    "objectID": "metagenomica.html#asignación-taxonomica",
    "href": "metagenomica.html#asignación-taxonomica",
    "title": "Introducción a Metagenomica",
    "section": "Asignación taxonomica",
    "text": "Asignación taxonomica\nLa asignación taxonómica implica comparar las secuencias de amplicones con las secuencias de referencia en la base de datos y determinar a qué taxones (por ejemplo, género, especie, familia, etc.) pertenecen las secuencias ver silva_db\n### Descarga de silva db\n\ndownload.file(\"https://zenodo.org/records/3986799/files/silva_nr99_v138_train_set.fa.gz?download=1\", \"data/reference/silva_nr99_v138_train_set.fa.gz\")\n\n## Asignación taxonomica\nruta_clasificador &lt;- \"data/reference/silva_nr99_v138_train_set.fa.gz\"\ntaxa &lt;- assignTaxonomy(seqtab_nochim, ruta_clasificador , multithread = TRUE)"
  },
  {
    "objectID": "metagenomica.html#inferencia-de-asv",
    "href": "metagenomica.html#inferencia-de-asv",
    "title": "Introducción a Metagenomica",
    "section": "Inferencia de ASV",
    "text": "Inferencia de ASV\n\nLa función dada() utiliza el modelo de errores para identificar y corregir errores en las secuencias de amplicones. Luego, se generan las Amplicon Sequence Variants (ASVs), que son secuencias únicas y precisas representativas de la diversidad en la muestra. Estas ASVs son el equivalente de los OTUs (Operational Taxonomic Units), pero con una mayor resolución y precisión.\n\ndadaF &lt;- dada(filtF, err = errF, multithread = T)\ndadaR &lt;- dada(filtR, err = errR, multithread = T)"
  },
  {
    "objectID": "metagenomica.html#creacion-de-objeto-phyloseq",
    "href": "metagenomica.html#creacion-de-objeto-phyloseq",
    "title": "Introducción a Metagenomica",
    "section": "Creacion de objeto Phyloseq",
    "text": "Creacion de objeto Phyloseq\n\nSe crea el objeto phyloseq_ob utilizando la función phyloseq(). Este objeto integra varios componentes esenciales para el análisis de microbiome, que incluyen:\n\notu_table: Representa la tabla de abundancia de las Amplicon Sequence Variants (ASVs) o similares. En este caso, se utiliza la variable seqtab_nochim para construir esta parte del objeto.\nsample_data: Contiene los metadatos asociados a cada muestra. En este caso, se utiliza la variable met para proporcionar esta información.\ntax_table: Almacena la información taxonómica asignada a las secuencias. Se utiliza la variable taxa para crear esta parte del objeto.\n\n\nlibrary(phyloseq)\n## uso de metadata\nmet &lt;- read.csv(\"data/Metadata.csv\", row.names = 1)\nmet &lt;- met[order(lecturas$name),]\n\n## apoyo phyloseq:generar tablas\nphyloseq_ob &lt;- phyloseq(otu_table(seqtab_nochim, taxa_are_rows = F),\n                        sample_data(met),\n                        tax_table(taxa))\n\nExportar datos de ASV\n## extraer nombres de los ASV\ndna &lt;- Biostrings::DNAStringSet(taxa_names(phyloseq_ob))\nnames(dna) &lt;- taxa_names(phyloseq_ob)\nps &lt;- merge_phyloseq(phyloseq_ob, dna)\ntaxa_names(ps) &lt;- paste0(\"ASV\", seq(ntaxa(ps)))\n\n## exportar secuencias representativas\n#exportar archivo fasta: futuros arboles filogeneticos\nnames(dna) &lt;- taxa_names(ps)\nwriteXStringSet(dna, \"results/rep-seq.fna\")\n\n\nExportar datos taxonomicos\n## exportar tabla de taxonomia\nTAX &lt;- as(tax_table(ps), \"matrix\")\nwrite.csv(TAX,\"results/taxa-gut16.csv\")\n\n## Exportar tablas de ASV\"s\nASV &lt;- as(otu_table(ps),\"matrix\")\nASV &lt;- t(ASV)\ncolnames(ASV) &lt;- gsub(\"_filt_1.fastq.gz\",\"\",colnames(ASV), fixed = TRUE)\nwrite.csv(ASV, \"results/asv-table.csv\")\n\n\nImportar datos y guardar Rdata\n### importar tablas de ASV\nasv_table &lt;- read.csv(\"results/asv-table.csv\", row.names = 1, header = T)\nclass(asv_table)\n### crear o convertir a tabla tipo phyloseq\nASV &lt;- otu_table(asv_table, taxa_are_rows = T)\nclass(ASV)\nASV\n### importar tabla de taxonomia\ntaxonomy &lt;- read.csv(file = \"results/taxa-gut16.csv\", header = T, row.names = 1)\n### Creando un objeto con los Taxa\nTAX &lt;- tax_table(as.matrix(taxonomy))\n### Creando un objeto physeq de prueba\nphyseq &lt;- phyloseq(ASV, TAX)\nrandom_tree = rtree(ntaxa(physeq), rooted=FALSE, tip.label=taxa_names(physeq))\n\n### Importar tabla de metadatos\nmetadata &lt;- read.csv(\"data/Metadata.csv\", header = T, row.names = 1)\n\n### Convertir tabla a tipo phyloseq\nMETA &lt;- sample_data(metadata)\n\n### construir phyloseq final\nps &lt;- merge_phyloseq(physeq, META, random_tree)\nps\nsaveRDS(ps, \"results/objeto_phyloseq.RDS\")\n\n\nCargar Rdata y rearefacción\nRarificación: - El término “rarefacción” se refiere a la práctica de estandarizar el número de secuencias o lecturas en todas las muestras de un conjunto de datos. Esto se hace para abordar desigualdades en la profundidad de secuenciación entre muestras, lo que puede influir en los análisis de diversidad y en la comparación entre muestras.\n\nps &lt;- readRDS(\"results/objeto_phyloseq.RDS\")\n\nset.seed(1)\nps_rar &lt;- rarefy_even_depth(ps, sample.size = 1200)\nsaveRDS(ps_rar,\"results/phyloseq_rar\")"
  },
  {
    "objectID": "metagenomica.html#gráficas-para-metagenómica",
    "href": "metagenomica.html#gráficas-para-metagenómica",
    "title": "Introducción a Metagenomica",
    "section": "Gráficas para metagenómica",
    "text": "Gráficas para metagenómica\nBiocManager::install(c(\"phyloseq\", \"microbiome\", \"ComplexHeatmap\"), update = FALSE)\ninstall.packages(\n  \"microViz\",\n  repos = c(davidbarnett = \"https://david-barnett.r-universe.dev\", getOption(\"repos\"))\n)\nlibrary(microViz)\n\nEn esta sección, se realiza una visualización de la abundancia relativa de los ASVs a nivel de género. Se genera un gráfico de barras utilizando la función comp_barplot(). El gráfico muestra los 12 géneros más abundantes en cada muestra y se utiliza merge_other = TRUE para agrupar los géneros menos abundantes en una categoría llamada “Other”.\n\n#importar objeto phyloseq (ps)\nps &lt;- readRDS(\"results/objeto_phyloseq.RDS\")\n\n## Corregir NA's\nnew_ps &lt;- ps %&gt;% tax_fix(unknowns = c(\"endosymbionts\"))\nnew_ps &lt;- ps %&gt;% tax_fix(unknowns = c(\"Unknown Family\"))\n\n## plot\n\nnew_ps %&gt;% \n  comp_barplot(\"Genus\", n_taxa = 12, merge_other = TRUE) +\n  facet_wrap(vars(location), scales = \"free\") + # scales = \"free\" is IMPORTANT!\n  coord_flip() +\n  ggtitle(\n    \"ABUNDANCIA RELATIVA DE ASV\",\n  ) +\n  theme(axis.ticks.y = element_blank(), strip.text = element_text(face = \"bold\"))\n\n\n\n\n\n\n\nResumen\n\n\n\n\n\nPasos de Ejecución del Tutorial:\n\nCargar Librerías y Scripts:\n\nSe cargan las librerías y scripts necesarios para el análisis.\n\nDescargar Secuencias:\n\nSe descargan las secuencias desde una fuente específica utilizando códigos de acceso.\n\nCalidad y Filtrado de Secuencias:\n\nSe evalúa la calidad de las secuencias y se filtran aquellas que no cumplen con los estándares de calidad.\nSe realiza un análisis de calidad de las secuencias.\nSe filtran las secuencias para eliminar aquellas de baja calidad.\n\nModelar los Errores de Secuenciación:\n\nSe modelan los errores de secuenciación para comprender y corregir los errores presentes en las secuencias.\n\nInferencia de ASV (Amplicon Sequence Variant):\n\nSe realiza la inferencia de las Amplicon Sequence Variants (ASVs), que representan secuencias precisas y únicas generadas a partir de datos de secuenciación.\n\nDenoising de Datos:\n\nSe realiza el proceso de denoising para eliminar ruido y duplicados de las secuencias, mejorando la calidad de los datos.\n\nAsignación Taxonómica:\n\nSe asigna una clasificación taxonómica a las secuencias utilizando una base de datos de referencia.\n\nCreación de Objeto Phyloseq para Análisis de Microbioma:\n\nSe crea un objeto Phyloseq que integra información sobre la tabla de abundancia de ASVs, metadatos y taxonomía.\n\nExportar Datos de ASV:\n\nSe exportan las secuencias representativas de los ASVs y la información taxonómica.\n\nImportar Datos y Guardar en Formato RDS para Reanálisis:\n\nSe importan los datos de ASVs y se almacenan en un formato RDS para futuros análisis.\n\nRealizar Rarificación para Igualar la Profundidad de Secuenciación entre Muestras:\n\nSe realiza la rarificación para estandarizar el número de secuencias en todas las muestras, abordando desigualdades en la profundidad de secuenciación.\n\nVisualizar la Abundancia Relativa de ASVs y Realizar Correcciones Taxonómicas:\n\nSe visualiza la abundancia relativa de ASVs a nivel de género y se aplican correcciones taxonómicas en caso de identificaciones incorrectas."
  },
  {
    "objectID": "index.html#instalación-de-r-y-rstudio-para-mac",
    "href": "index.html#instalación-de-r-y-rstudio-para-mac",
    "title": "Instalación de Paquetes",
    "section": "Instalación de R y Rstudio para Mac",
    "text": "Instalación de R y Rstudio para Mac\n\n\n\n\n\n\n\nAdvertencia\n\n\n\nPara la instalar R y R studio en Mac debemos considerar que tipo de GPU tienen su Mac, si tiene M1/M2 o esta basado en Intel."
  },
  {
    "objectID": "modulo6.html#instalación-y-carga-de-dplyr",
    "href": "modulo6.html#instalación-y-carga-de-dplyr",
    "title": "Introducción a R",
    "section": "Instalación y Carga de dplyr",
    "text": "Instalación y Carga de dplyr\nAntes de comenzar, asegúrate de tener instalado el paquete dplyr. Si aún no lo has hecho, puedes instalarlo y cargarlo de la siguiente manera:\ninstall.packages(\"dplyr\")\n#install.packages(\"tidyverse\")\nlibrary(dplyr)",
    "crumbs": [
      "Introduccion a R",
      "Introducción a R"
    ]
  },
  {
    "objectID": "modulo8.1.html#descarga-de-archivos",
    "href": "modulo8.1.html#descarga-de-archivos",
    "title": "Pipeline de Genómica en R",
    "section": "Descarga de archivos",
    "text": "Descarga de archivos\n\n\n\n\n\n\nEn resumen\n\n\n\nEl código descarga archivos SRA utilizando los identificadores específicos proporcionados (SRR15616379 y SRR26204001) y luego los convierte en archivos FASTQ en el directorio especificado. Ver Aqui",
    "crumbs": [
      "Fundamentos NGS",
      "Pipeline de Genómica en R"
    ]
  }
]